{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:08:12.308019Z",
     "start_time": "2019-06-26T11:08:08.460722Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/hsiehch/ECG-project/experimental/tools_stable.py:12: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/opt/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/opt/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1426, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/lib/python3.6/asyncio/events.py\", line 127, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-9c2c8afca2e1>\", line 6, in <module>\n",
      "    import tools_stable as tools\n",
      "  File \"/home/hsiehch/ECG-project/experimental/tools_stable.py\", line 1, in <module>\n",
      "    import pylab as plt\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/pylab.py\", line 1, in <module>\n",
      "    from matplotlib.pylab import *\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/matplotlib/pylab.py\", line 252, in <module>\n",
      "    from matplotlib import cbook, mlab, pyplot as plt\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 71, in <module>\n",
      "    from matplotlib.backends import pylab_setup\n",
      "  File \"/opt/anaconda3/lib/python3.6/site-packages/matplotlib/backends/__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use('Agg')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: (10151, 9000, 1)\n",
      "Train Label:  (10151, 4)\n",
      "KFold(n_splits=5, random_state=100, shuffle=True)\n",
      "trian: [    0     1     2 ... 10148 10149 10150] len 8120 test: [   12    21    22 ... 10136 10137 10138] len 2031\n",
      "trian: [    2     4     5 ... 10148 10149 10150] len 8121 test: [    0     1     3 ... 10143 10144 10147] len 2030\n",
      "trian: [    0     1     2 ... 10148 10149 10150] len 8121 test: [    4     5     8 ... 10130 10133 10146] len 2030\n",
      "trian: [    0     1     2 ... 10146 10147 10149] len 8121 test: [   17    23    26 ... 10135 10148 10150] len 2030\n",
      "trian: [    0     1     3 ... 10147 10148 10150] len 8121 test: [    2     6     7 ... 10142 10145 10149] len 2030\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import f1_score\n",
    "import tools_stable as tools\n",
    "import plot_confusion_matrix_Copy1 as plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "training_data, training_label, validation_data, validation_label, validation_cate_label = tools.get_data()\n",
    "\n",
    "ks = 5\n",
    "num_layer = 10\n",
    "bs = 30\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tt=[]\n",
    "for i in training_label[0]:\n",
    "    tt.append(np.argmax(i))\n",
    "\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "2031\n",
      "1.033050298690796\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import time\n",
    "def run(bs, lr, ks, num_layer):\n",
    "    fold=1\n",
    "    for index, (X_train, Y_train, X_val, Y_val, val_cat) in enumerate(zip(training_data,\n",
    "                                                       training_label,\n",
    "                                                       validation_data,\n",
    "                                                       validation_label,\n",
    "                                                       validation_cate_label)):\n",
    "        if index != 0: continue\n",
    "    \n",
    "        print(\"Fold \"+str(fold))\n",
    "\n",
    "        model = load_model('K-1_model.h5')\n",
    "#         evaluation = model.evaluate(x = X_val, y = Y_val)\n",
    "        print(len(X_val))\n",
    "        t1 = time.time()\n",
    "        validation_prediction = model.predict_classes(X_val, batch_size=bs)\n",
    "        print(time.time()-t1)\n",
    "        \n",
    "    \n",
    "run(bs, lr, ks, num_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "confusion matrix(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T14:45:55.291782Z",
     "start_time": "2019-06-27T14:45:54.878037Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plot_confusion_matrix_Copy1 as plot_confusion_matrix\n",
    "cnf_matrix = np.array([[142, 3, 3, 21],[8, 1127, 15, 65], [6, 7, 35, 5], [27, 119, 14, 434]])\n",
    "plot_confusion_matrix.plot_confusion_matrix(cnf_matrix, classes=['AF','Normal','Noisy','Other'], save_png=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:15:32.693289Z",
     "start_time": "2019-06-26T11:08:37.399476Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Train on 8120 samples, validate on 2031 samples\n",
      "Epoch 1/100\n",
      "8120/8120 [==============================] - 13s 2ms/step - loss: 1.0014 - acc: 0.5756 - val_loss: 0.9492 - val_acc: 0.5707\n",
      "Epoch 2/100\n",
      "8120/8120 [==============================] - 8s 950us/step - loss: 0.9077 - acc: 0.6106 - val_loss: 0.9003 - val_acc: 0.6056\n",
      "Epoch 3/100\n",
      "8120/8120 [==============================] - 8s 963us/step - loss: 0.8609 - acc: 0.6384 - val_loss: 0.8491 - val_acc: 0.6332\n",
      "Epoch 4/100\n",
      "8120/8120 [==============================] - 8s 967us/step - loss: 0.8002 - acc: 0.6722 - val_loss: 0.8087 - val_acc: 0.6460\n",
      "Epoch 5/100\n",
      "8120/8120 [==============================] - 8s 980us/step - loss: 0.7450 - acc: 0.6958 - val_loss: 0.7517 - val_acc: 0.6691\n",
      "Epoch 6/100\n",
      "8120/8120 [==============================] - 8s 976us/step - loss: 0.6896 - acc: 0.7180 - val_loss: 0.6817 - val_acc: 0.7218\n",
      "Epoch 7/100\n",
      "8120/8120 [==============================] - 8s 975us/step - loss: 0.6399 - acc: 0.7429 - val_loss: 0.6494 - val_acc: 0.7120\n",
      "Epoch 8/100\n",
      "8120/8120 [==============================] - 8s 964us/step - loss: 0.6007 - acc: 0.7517 - val_loss: 0.6611 - val_acc: 0.7302\n",
      "Epoch 9/100\n",
      "8120/8120 [==============================] - 8s 979us/step - loss: 0.5630 - acc: 0.7749 - val_loss: 0.5973 - val_acc: 0.7582\n",
      "Epoch 10/100\n",
      "8120/8120 [==============================] - 8s 987us/step - loss: 0.5352 - acc: 0.7900 - val_loss: 0.5376 - val_acc: 0.7917\n",
      "Epoch 11/100\n",
      "8120/8120 [==============================] - 8s 981us/step - loss: 0.5146 - acc: 0.7998 - val_loss: 0.5673 - val_acc: 0.7809\n",
      "Epoch 12/100\n",
      "8120/8120 [==============================] - 8s 986us/step - loss: 0.5012 - acc: 0.8103 - val_loss: 0.6094 - val_acc: 0.7873\n",
      "Epoch 13/100\n",
      "8120/8120 [==============================] - 8s 992us/step - loss: 0.4834 - acc: 0.8144 - val_loss: 0.4923 - val_acc: 0.8095\n",
      "Epoch 14/100\n",
      "8120/8120 [==============================] - 8s 993us/step - loss: 0.4763 - acc: 0.8185 - val_loss: 0.5252 - val_acc: 0.7873\n",
      "Epoch 15/100\n",
      "8120/8120 [==============================] - 8s 995us/step - loss: 0.4592 - acc: 0.8328 - val_loss: 0.5364 - val_acc: 0.8090\n",
      "Epoch 16/100\n",
      "8120/8120 [==============================] - 8s 993us/step - loss: 0.4506 - acc: 0.8328 - val_loss: 0.5135 - val_acc: 0.8119\n",
      "Epoch 17/100\n",
      "8120/8120 [==============================] - 8s 991us/step - loss: 0.4402 - acc: 0.8408 - val_loss: 0.4742 - val_acc: 0.8218\n",
      "Epoch 18/100\n",
      "8120/8120 [==============================] - 8s 989us/step - loss: 0.4326 - acc: 0.8365 - val_loss: 0.4685 - val_acc: 0.8277\n",
      "Epoch 19/100\n",
      "8120/8120 [==============================] - 8s 984us/step - loss: 0.4222 - acc: 0.8440 - val_loss: 0.4619 - val_acc: 0.8237\n",
      "Epoch 20/100\n",
      "8120/8120 [==============================] - 8s 991us/step - loss: 0.4143 - acc: 0.8515 - val_loss: 0.4576 - val_acc: 0.8262\n",
      "Epoch 21/100\n",
      "8120/8120 [==============================] - 8s 994us/step - loss: 0.3958 - acc: 0.8518 - val_loss: 0.4757 - val_acc: 0.8326\n",
      "Epoch 22/100\n",
      "8120/8120 [==============================] - 8s 984us/step - loss: 0.3953 - acc: 0.8550 - val_loss: 0.4884 - val_acc: 0.8301\n",
      "Epoch 23/100\n",
      "8120/8120 [==============================] - 8s 994us/step - loss: 0.3878 - acc: 0.8623 - val_loss: 0.4741 - val_acc: 0.8341\n",
      "Epoch 24/100\n",
      "8120/8120 [==============================] - 8s 997us/step - loss: 0.3734 - acc: 0.8651 - val_loss: 0.4543 - val_acc: 0.8415\n",
      "Epoch 25/100\n",
      "8120/8120 [==============================] - 8s 995us/step - loss: 0.3710 - acc: 0.8651 - val_loss: 0.5277 - val_acc: 0.8291\n",
      "Epoch 26/100\n",
      "8120/8120 [==============================] - 8s 995us/step - loss: 0.3691 - acc: 0.8686 - val_loss: 0.4579 - val_acc: 0.8365\n",
      "Epoch 27/100\n",
      "8120/8120 [==============================] - 8s 984us/step - loss: 0.3572 - acc: 0.8691 - val_loss: 0.4484 - val_acc: 0.8415\n",
      "Epoch 28/100\n",
      "8120/8120 [==============================] - 8s 1000us/step - loss: 0.3546 - acc: 0.8722 - val_loss: 0.4610 - val_acc: 0.8277\n",
      "Epoch 29/100\n",
      "8120/8120 [==============================] - 8s 991us/step - loss: 0.3470 - acc: 0.8749 - val_loss: 0.4543 - val_acc: 0.8439\n",
      "Epoch 30/100\n",
      "8120/8120 [==============================] - 8s 978us/step - loss: 0.3387 - acc: 0.8792 - val_loss: 0.4370 - val_acc: 0.8415\n",
      "Epoch 31/100\n",
      "8120/8120 [==============================] - 8s 993us/step - loss: 0.3298 - acc: 0.8819 - val_loss: 0.4898 - val_acc: 0.8395\n",
      "Epoch 32/100\n",
      "8120/8120 [==============================] - 8s 976us/step - loss: 0.3221 - acc: 0.8858 - val_loss: 0.4541 - val_acc: 0.8424\n",
      "Epoch 33/100\n",
      "8120/8120 [==============================] - 8s 987us/step - loss: 0.3159 - acc: 0.8847 - val_loss: 0.4760 - val_acc: 0.8405\n",
      "Epoch 34/100\n",
      "8120/8120 [==============================] - 8s 995us/step - loss: 0.3159 - acc: 0.8872 - val_loss: 0.4727 - val_acc: 0.8370\n",
      "Epoch 35/100\n",
      "8120/8120 [==============================] - 8s 974us/step - loss: 0.3080 - acc: 0.8947 - val_loss: 0.4749 - val_acc: 0.8419\n",
      "Epoch 36/100\n",
      "8120/8120 [==============================] - 8s 989us/step - loss: 0.2931 - acc: 0.8938 - val_loss: 0.5092 - val_acc: 0.8331\n",
      "Epoch 37/100\n",
      "8120/8120 [==============================] - 8s 986us/step - loss: 0.2843 - acc: 0.9032 - val_loss: 0.4679 - val_acc: 0.8479\n",
      "Epoch 38/100\n",
      "8120/8120 [==============================] - 8s 984us/step - loss: 0.2815 - acc: 0.9005 - val_loss: 0.5036 - val_acc: 0.8370\n",
      "Epoch 39/100\n",
      "8120/8120 [==============================] - 8s 976us/step - loss: 0.2814 - acc: 0.8988 - val_loss: 0.4699 - val_acc: 0.8375\n",
      "Epoch 40/100\n",
      "8120/8120 [==============================] - 8s 986us/step - loss: 0.2692 - acc: 0.9039 - val_loss: 0.5313 - val_acc: 0.8365\n",
      "Epoch 41/100\n",
      "8120/8120 [==============================] - 8s 965us/step - loss: 0.2569 - acc: 0.9085 - val_loss: 0.5329 - val_acc: 0.8454\n",
      "Epoch 42/100\n",
      "8120/8120 [==============================] - 8s 995us/step - loss: 0.2602 - acc: 0.9131 - val_loss: 0.4954 - val_acc: 0.8424\n",
      "Epoch 43/100\n",
      "8120/8120 [==============================] - 8s 989us/step - loss: 0.2568 - acc: 0.9096 - val_loss: 0.5043 - val_acc: 0.8429\n",
      "Epoch 44/100\n",
      "8120/8120 [==============================] - 8s 979us/step - loss: 0.2546 - acc: 0.9127 - val_loss: 0.4974 - val_acc: 0.8395\n",
      "Epoch 45/100\n",
      "8120/8120 [==============================] - 8s 989us/step - loss: 0.2398 - acc: 0.9197 - val_loss: 0.5231 - val_acc: 0.8395\n",
      "Epoch 46/100\n",
      "8120/8120 [==============================] - 8s 971us/step - loss: 0.2461 - acc: 0.9147 - val_loss: 0.5071 - val_acc: 0.8380\n",
      "Epoch 47/100\n",
      "8120/8120 [==============================] - 8s 978us/step - loss: 0.2376 - acc: 0.9166 - val_loss: 0.4806 - val_acc: 0.8474\n",
      "Epoch 48/100\n",
      "8120/8120 [==============================] - 8s 987us/step - loss: 0.2390 - acc: 0.9142 - val_loss: 0.4856 - val_acc: 0.8429\n",
      "Epoch 49/100\n",
      "8120/8120 [==============================] - 8s 999us/step - loss: 0.2289 - acc: 0.9177 - val_loss: 0.5238 - val_acc: 0.8484\n",
      "Epoch 50/100\n",
      "8120/8120 [==============================] - 8s 992us/step - loss: 0.2168 - acc: 0.9238 - val_loss: 0.5349 - val_acc: 0.8449\n",
      "2031/2031 [==============================] - 1s 321us/step\n",
      "[0.79444444 0.6440678  0.90325271 0.76222597]\n"
     ]
    }
   ],
   "source": [
    "def run(bs, lr, ks, num_layer):\n",
    "    fold=1\n",
    "    for index, (X_train, Y_train, X_val, Y_val, val_cat) in enumerate(zip(training_data,\n",
    "                                                       training_label,\n",
    "                                                       validation_data,\n",
    "                                                       validation_label,\n",
    "                                                       validation_cate_label)):\n",
    "        if index != 0: continue\n",
    "        model = tools.create_model(lr, bs, ks, num_layer)\n",
    "        print(\"Fold \"+str(fold))\n",
    "        early_stop = EarlyStopping(patience=20)\n",
    "        history = model.fit(x = X_train, \n",
    "                            y = Y_train,\n",
    "                            epochs=100,\n",
    "                            validation_data=(X_val, Y_val),\n",
    "                            callbacks=[early_stop],\n",
    "                            batch_size=bs, \n",
    "                            verbose=1)\n",
    "        model.save('K-1_model.h5')\n",
    "#         model = load_model('K-1_model.h5')\n",
    "        evaluation = model.evaluate(x = X_val, y = Y_val)\n",
    "        validation_prediction = model.predict_classes(X_val, batch_size=bs)\n",
    "        score = f1_score(val_cat, validation_prediction, average=None)\n",
    "        print(score)\n",
    "        \n",
    "        fold = fold + 1\n",
    "        \n",
    "        test_prediction = model.predict_classes(X_val, batch_size=1)\n",
    "        cnf_matrix = confusion_matrix(val_cat, test_prediction)\n",
    "        plot_confusion_matrix.plot_confusion_matrix(cnf_matrix, classes=['AF','Noise','Normal','Other'], save_png=True)\n",
    "        \n",
    "        return X_val, val_cat, validation_prediction\n",
    "    \n",
    "f1_X_val, f1_val_cat, f1_validation_prediction = run(bs, lr, ks, num_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:54:18.920157Z",
     "start_time": "2019-06-26T11:54:18.913984Z"
    }
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "def show(rec, gt, pred):\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.xlim((0,3000))\n",
    "    plt.plot(rec)\n",
    "    print(\"GT:{}\".format(gt))\n",
    "    print(\"Pred: {}\".format(pred))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'A':0, '~':1, 'N':2, 'O':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T16:22:46.014329Z",
     "start_time": "2019-06-19T16:15:26.064861Z"
    },
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Train on 8120 samples, validate on 2031 samples\n",
      "Epoch 1/100\n",
      "8120/8120 [==============================] - 13s 2ms/step - loss: 1.0232 - acc: 0.5762 - val_loss: 0.9721 - val_acc: 0.5982\n",
      "Epoch 2/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.9253 - acc: 0.6006 - val_loss: 0.8476 - val_acc: 0.6420\n",
      "Epoch 3/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.8556 - acc: 0.6353 - val_loss: 0.8118 - val_acc: 0.6603\n",
      "Epoch 4/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.7979 - acc: 0.6583 - val_loss: 0.7275 - val_acc: 0.6864\n",
      "Epoch 5/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.7442 - acc: 0.6858 - val_loss: 0.6654 - val_acc: 0.7139\n",
      "Epoch 6/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.7115 - acc: 0.6963 - val_loss: 0.6361 - val_acc: 0.7292\n",
      "Epoch 7/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.6723 - acc: 0.7182 - val_loss: 0.5782 - val_acc: 0.7637\n",
      "Epoch 8/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.6170 - acc: 0.7466 - val_loss: 0.5843 - val_acc: 0.7582\n",
      "Epoch 9/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.5919 - acc: 0.7578 - val_loss: 0.5288 - val_acc: 0.7784\n",
      "Epoch 10/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.5587 - acc: 0.7796 - val_loss: 0.5857 - val_acc: 0.7661\n",
      "Epoch 11/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.5485 - acc: 0.7852 - val_loss: 0.5216 - val_acc: 0.8085\n",
      "Epoch 12/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.5265 - acc: 0.7995 - val_loss: 0.4874 - val_acc: 0.8188\n",
      "Epoch 13/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.5055 - acc: 0.8038 - val_loss: 0.4643 - val_acc: 0.8287\n",
      "Epoch 14/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.4945 - acc: 0.8135 - val_loss: 0.4568 - val_acc: 0.8237\n",
      "Epoch 15/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.4840 - acc: 0.8218 - val_loss: 0.4664 - val_acc: 0.8257\n",
      "Epoch 16/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.4655 - acc: 0.8270 - val_loss: 0.4498 - val_acc: 0.8336\n",
      "Epoch 17/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.4559 - acc: 0.8312 - val_loss: 0.4394 - val_acc: 0.8380\n",
      "Epoch 18/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.4443 - acc: 0.8365 - val_loss: 0.4494 - val_acc: 0.8355\n",
      "Epoch 19/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.4366 - acc: 0.8472 - val_loss: 0.4749 - val_acc: 0.8365\n",
      "Epoch 20/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.4321 - acc: 0.8415 - val_loss: 0.4301 - val_acc: 0.8439\n",
      "Epoch 21/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.4149 - acc: 0.8523 - val_loss: 0.4387 - val_acc: 0.8321\n",
      "Epoch 22/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.4162 - acc: 0.8480 - val_loss: 0.4145 - val_acc: 0.8498\n",
      "Epoch 23/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.4019 - acc: 0.8538 - val_loss: 0.4136 - val_acc: 0.8488\n",
      "Epoch 24/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.3987 - acc: 0.8538 - val_loss: 0.4181 - val_acc: 0.8424\n",
      "Epoch 25/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.3817 - acc: 0.8596 - val_loss: 0.4081 - val_acc: 0.8528\n",
      "Epoch 26/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.3783 - acc: 0.8612 - val_loss: 0.4118 - val_acc: 0.8484\n",
      "Epoch 27/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.3692 - acc: 0.8666 - val_loss: 0.4045 - val_acc: 0.8513\n",
      "Epoch 28/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.3741 - acc: 0.8644 - val_loss: 0.4222 - val_acc: 0.8459\n",
      "Epoch 29/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.3512 - acc: 0.8736 - val_loss: 0.4052 - val_acc: 0.8533\n",
      "Epoch 30/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.3578 - acc: 0.8738 - val_loss: 0.4548 - val_acc: 0.8488\n",
      "Epoch 31/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.3464 - acc: 0.8739 - val_loss: 0.4130 - val_acc: 0.8498\n",
      "Epoch 32/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.3361 - acc: 0.8817 - val_loss: 0.4283 - val_acc: 0.8424\n",
      "Epoch 33/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.3327 - acc: 0.8807 - val_loss: 0.4274 - val_acc: 0.8513\n",
      "Epoch 34/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.3236 - acc: 0.8830 - val_loss: 0.4264 - val_acc: 0.8508\n",
      "Epoch 35/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.3277 - acc: 0.8821 - val_loss: 0.4363 - val_acc: 0.8355\n",
      "Epoch 36/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.3134 - acc: 0.8837 - val_loss: 0.4056 - val_acc: 0.8577\n",
      "Epoch 37/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.3085 - acc: 0.8898 - val_loss: 0.4269 - val_acc: 0.8557\n",
      "Epoch 38/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.2958 - acc: 0.8931 - val_loss: 0.4148 - val_acc: 0.8518\n",
      "Epoch 39/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.2873 - acc: 0.9025 - val_loss: 0.4435 - val_acc: 0.8577\n",
      "Epoch 40/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.2837 - acc: 0.9011 - val_loss: 0.4427 - val_acc: 0.8419\n",
      "Epoch 41/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.2756 - acc: 0.8995 - val_loss: 0.4543 - val_acc: 0.8419\n",
      "Epoch 42/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.2719 - acc: 0.9086 - val_loss: 0.4480 - val_acc: 0.8479\n",
      "Epoch 43/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.2665 - acc: 0.9075 - val_loss: 0.4395 - val_acc: 0.8567\n",
      "Epoch 44/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.2611 - acc: 0.9080 - val_loss: 0.4401 - val_acc: 0.8469\n",
      "Epoch 45/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.2475 - acc: 0.9112 - val_loss: 0.4407 - val_acc: 0.8518\n",
      "Epoch 46/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.2502 - acc: 0.9091 - val_loss: 0.4517 - val_acc: 0.8552\n",
      "Epoch 47/100\n",
      "8120/8120 [==============================] - 9s 1ms/step - loss: 0.2393 - acc: 0.9181 - val_loss: 0.4753 - val_acc: 0.8572\n",
      "2031/2031 [==============================] - 1s 346us/step\n",
      "[0.79768786 0.63414634 0.91396104 0.77590788]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "def run(bs, lr, ks, num_layer):\n",
    "    fold=1\n",
    "    for index, (X_train, Y_train, X_val, Y_val, val_cat) in enumerate(zip(training_data,\n",
    "                                                       training_label,\n",
    "                                                       validation_data,\n",
    "                                                       validation_label,\n",
    "                                                       validation_cate_label)):\n",
    "        if index != 0: continue\n",
    "        train_cat = [np.where(r==1) for r in Y_train]\n",
    "        cat = [r[0].tolist()[0] for r in train_cat]\n",
    "        class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(cat),\n",
    "                                                 cat)\n",
    "        model = tools.create_model(lr, bs, ks, num_layer)\n",
    "        print(\"Fold \"+str(fold))\n",
    "        early_stop = EarlyStopping(patience=20)\n",
    "        history = model.fit(x = X_train, \n",
    "                            y = Y_train,\n",
    "                            epochs=100,\n",
    "                            validation_data=(X_val, Y_val),\n",
    "                            callbacks=[early_stop],\n",
    "                            batch_size=bs,\n",
    "                            class_weight=class_weights,\n",
    "                            verbose=1)\n",
    "        evaluation = model.evaluate(x = X_val, y = Y_val)\n",
    "        validation_prediction = model.predict_classes(X_val, batch_size=bs)\n",
    "        score = f1_score(val_cat, validation_prediction, average=None)\n",
    "        print(score)\n",
    "        \n",
    "#         tools.show_plot(inner_path, history)\n",
    "        fold = fold + 1\n",
    "        \n",
    "        test_prediction = model.predict_classes(X_val, batch_size=1)\n",
    "        cnf_matrix = confusion_matrix(val_cat, test_prediction)\n",
    "        plot_confusion_matrix.plot_confusion_matrix(cnf_matrix, classes=['AF','Noise','Normal','Other'], save_png=True)\n",
    "        \n",
    "        return X_val, val_cat, validation_prediction\n",
    "    \n",
    "f11_X_val, f11_val_cat, f11_validation_prediction = run(bs, lr, ks, num_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T12:45:55.484046Z",
     "start_time": "2019-06-19T12:39:24.132343Z"
    },
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Train on 8121 samples, validate on 2030 samples\n",
      "Epoch 1/100\n",
      "8121/8121 [==============================] - 11s 1ms/step - loss: 1.0392 - acc: 0.5741 - val_loss: 1.0143 - val_acc: 0.5906\n",
      "Epoch 2/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.9182 - acc: 0.5946 - val_loss: 0.8513 - val_acc: 0.6374\n",
      "Epoch 3/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.8639 - acc: 0.6305 - val_loss: 0.8429 - val_acc: 0.6709\n",
      "Epoch 4/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.8006 - acc: 0.6526 - val_loss: 0.7527 - val_acc: 0.6744\n",
      "Epoch 5/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.7536 - acc: 0.6822 - val_loss: 0.7044 - val_acc: 0.6852\n",
      "Epoch 6/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.6994 - acc: 0.7062 - val_loss: 0.6558 - val_acc: 0.7108\n",
      "Epoch 7/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.6485 - acc: 0.7308 - val_loss: 0.6635 - val_acc: 0.7187\n",
      "Epoch 8/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.6177 - acc: 0.7438 - val_loss: 0.6127 - val_acc: 0.7522\n",
      "Epoch 9/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.5826 - acc: 0.7631 - val_loss: 0.5339 - val_acc: 0.7911\n",
      "Epoch 10/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.5419 - acc: 0.7841 - val_loss: 0.5140 - val_acc: 0.8034\n",
      "Epoch 11/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.5261 - acc: 0.7914 - val_loss: 0.5158 - val_acc: 0.8133\n",
      "Epoch 12/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.5050 - acc: 0.8025 - val_loss: 0.4762 - val_acc: 0.8197\n",
      "Epoch 13/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4955 - acc: 0.8116 - val_loss: 0.4685 - val_acc: 0.8236\n",
      "Epoch 14/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4723 - acc: 0.8233 - val_loss: 0.4752 - val_acc: 0.8207\n",
      "Epoch 15/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4580 - acc: 0.8274 - val_loss: 0.5342 - val_acc: 0.8172\n",
      "Epoch 16/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4481 - acc: 0.8313 - val_loss: 0.4756 - val_acc: 0.8167\n",
      "Epoch 17/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4314 - acc: 0.8393 - val_loss: 0.4478 - val_acc: 0.8369\n",
      "Epoch 18/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4226 - acc: 0.8466 - val_loss: 0.4601 - val_acc: 0.8261\n",
      "Epoch 19/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4174 - acc: 0.8479 - val_loss: 0.4641 - val_acc: 0.8246\n",
      "Epoch 20/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4114 - acc: 0.8506 - val_loss: 0.4529 - val_acc: 0.8330\n",
      "Epoch 21/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3959 - acc: 0.8521 - val_loss: 0.4525 - val_acc: 0.8409\n",
      "Epoch 22/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3947 - acc: 0.8515 - val_loss: 0.4357 - val_acc: 0.8369\n",
      "Epoch 23/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3750 - acc: 0.8645 - val_loss: 0.4380 - val_acc: 0.8355\n",
      "Epoch 24/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3776 - acc: 0.8610 - val_loss: 0.4507 - val_acc: 0.8271\n",
      "Epoch 25/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3732 - acc: 0.8634 - val_loss: 0.4449 - val_acc: 0.8409\n",
      "Epoch 26/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3590 - acc: 0.8698 - val_loss: 0.4568 - val_acc: 0.8369\n",
      "Epoch 27/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3564 - acc: 0.8697 - val_loss: 0.4559 - val_acc: 0.8453\n",
      "Epoch 28/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3430 - acc: 0.8743 - val_loss: 0.4425 - val_acc: 0.8433\n",
      "Epoch 29/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3493 - acc: 0.8772 - val_loss: 0.4422 - val_acc: 0.8404\n",
      "Epoch 30/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3331 - acc: 0.8823 - val_loss: 0.4758 - val_acc: 0.8355\n",
      "Epoch 31/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3267 - acc: 0.8839 - val_loss: 0.4712 - val_acc: 0.8379\n",
      "Epoch 32/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3194 - acc: 0.8877 - val_loss: 0.4498 - val_acc: 0.8448\n",
      "Epoch 33/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3054 - acc: 0.8876 - val_loss: 0.4816 - val_acc: 0.8330\n",
      "Epoch 34/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3037 - acc: 0.8915 - val_loss: 0.4501 - val_acc: 0.8438\n",
      "Epoch 35/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2997 - acc: 0.8909 - val_loss: 0.5039 - val_acc: 0.8345\n",
      "Epoch 36/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2914 - acc: 0.8963 - val_loss: 0.5082 - val_acc: 0.8365\n",
      "Epoch 37/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2853 - acc: 0.9008 - val_loss: 0.4696 - val_acc: 0.8404\n",
      "Epoch 38/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2832 - acc: 0.8972 - val_loss: 0.4722 - val_acc: 0.8355\n",
      "Epoch 39/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2719 - acc: 0.9021 - val_loss: 0.5210 - val_acc: 0.8374\n",
      "Epoch 40/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2741 - acc: 0.9054 - val_loss: 0.4725 - val_acc: 0.8453\n",
      "Epoch 41/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2614 - acc: 0.9076 - val_loss: 0.4803 - val_acc: 0.8429\n",
      "Epoch 42/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2594 - acc: 0.9056 - val_loss: 0.4642 - val_acc: 0.8424\n",
      "2030/2030 [==============================] - 1s 387us/step\n",
      "[0.78238342 0.67368421 0.90346232 0.74377224]\n"
     ]
    }
   ],
   "source": [
    "def run(bs, lr, ks, num_layer):\n",
    "    \n",
    "    for index, (X_train, Y_train, X_val, Y_val, val_cat) in enumerate(zip(training_data,\n",
    "                                                       training_label,\n",
    "                                                       validation_data,\n",
    "                                                       validation_label,\n",
    "                                                       validation_cate_label)):\n",
    "        if index != 1: continue\n",
    "        model = tools.create_model(lr, bs, ks, num_layer)\n",
    "        print(\"Fold \"+str(index))\n",
    "        early_stop = EarlyStopping(patience=20)\n",
    "        history = model.fit(x = X_train, \n",
    "                            y = Y_train,\n",
    "                            epochs=100,\n",
    "                            validation_data=(X_val, Y_val),\n",
    "                            callbacks=[early_stop],\n",
    "                            batch_size=bs, \n",
    "                            verbose=1)\n",
    "        evaluation = model.evaluate(x = X_val, y = Y_val)\n",
    "        validation_prediction = model.predict_classes(X_val, batch_size=bs)\n",
    "        score = f1_score(val_cat, validation_prediction, average=None)\n",
    "        print(score)\n",
    "        \n",
    "#         tools.show_plot(inner_path, history)\n",
    "        \n",
    "        test_prediction = model.predict_classes(X_val, batch_size=1)\n",
    "        cnf_matrix = confusion_matrix(val_cat, test_prediction)\n",
    "        plot_confusion_matrix.plot_confusion_matrix(cnf_matrix, classes=['AF','Noise','Normal','Other'], save_png=True)\n",
    "        \n",
    "        return X_val, val_cat, validation_prediction\n",
    "    \n",
    "f2_X_val, f2_val_cat, f2_validation_prediction = run(bs, lr, ks, num_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T13:55:43.739064Z",
     "start_time": "2019-06-19T13:46:55.364012Z"
    },
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2\n",
      "Train on 8121 samples, validate on 2030 samples\n",
      "Epoch 1/100\n",
      "8121/8121 [==============================] - 12s 1ms/step - loss: 1.0100 - acc: 0.5754 - val_loss: 0.9561 - val_acc: 0.5734\n",
      "Epoch 2/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.8907 - acc: 0.6100 - val_loss: 0.9090 - val_acc: 0.6488\n",
      "Epoch 3/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.8281 - acc: 0.6436 - val_loss: 0.8060 - val_acc: 0.6650\n",
      "Epoch 4/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.7777 - acc: 0.6725 - val_loss: 0.9151 - val_acc: 0.6197\n",
      "Epoch 5/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.7338 - acc: 0.6909 - val_loss: 0.7570 - val_acc: 0.6704\n",
      "Epoch 6/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.6905 - acc: 0.7075 - val_loss: 0.6779 - val_acc: 0.7158\n",
      "Epoch 7/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.6425 - acc: 0.7285 - val_loss: 0.6387 - val_acc: 0.7394\n",
      "Epoch 8/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.6032 - acc: 0.7547 - val_loss: 0.6222 - val_acc: 0.7453\n",
      "Epoch 9/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.5688 - acc: 0.7764 - val_loss: 0.6597 - val_acc: 0.7320\n",
      "Epoch 10/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.5494 - acc: 0.7836 - val_loss: 0.5752 - val_acc: 0.7754\n",
      "Epoch 11/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.5338 - acc: 0.7857 - val_loss: 0.5674 - val_acc: 0.7704\n",
      "Epoch 12/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.5215 - acc: 0.8000 - val_loss: 0.5283 - val_acc: 0.8030\n",
      "Epoch 13/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4927 - acc: 0.8089 - val_loss: 0.5211 - val_acc: 0.8049\n",
      "Epoch 14/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4782 - acc: 0.8182 - val_loss: 0.5376 - val_acc: 0.8030\n",
      "Epoch 15/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4716 - acc: 0.8226 - val_loss: 0.5642 - val_acc: 0.7862\n",
      "Epoch 16/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4627 - acc: 0.8244 - val_loss: 0.4751 - val_acc: 0.8222\n",
      "Epoch 17/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4473 - acc: 0.8347 - val_loss: 0.4822 - val_acc: 0.8158\n",
      "Epoch 18/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4381 - acc: 0.8378 - val_loss: 0.4949 - val_acc: 0.8217\n",
      "Epoch 19/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4253 - acc: 0.8402 - val_loss: 0.4760 - val_acc: 0.8286\n",
      "Epoch 20/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4205 - acc: 0.8416 - val_loss: 0.4828 - val_acc: 0.8256\n",
      "Epoch 21/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4081 - acc: 0.8522 - val_loss: 0.4662 - val_acc: 0.8315\n",
      "Epoch 22/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3149 - acc: 0.8868 - val_loss: 0.4712 - val_acc: 0.8379\n",
      "Epoch 34/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3173 - acc: 0.8879 - val_loss: 0.4949 - val_acc: 0.8320\n",
      "Epoch 35/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3046 - acc: 0.8935 - val_loss: 0.4848 - val_acc: 0.8315\n",
      "Epoch 36/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3001 - acc: 0.8947 - val_loss: 0.4924 - val_acc: 0.8379\n",
      "Epoch 37/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2894 - acc: 0.8964 - val_loss: 0.5081 - val_acc: 0.8300\n",
      "Epoch 38/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2888 - acc: 0.8964 - val_loss: 0.4630 - val_acc: 0.8394\n",
      "Epoch 39/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2773 - acc: 0.9021 - val_loss: 0.5103 - val_acc: 0.8443\n",
      "Epoch 40/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2664 - acc: 0.9053 - val_loss: 0.5399 - val_acc: 0.8429\n",
      "Epoch 41/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2741 - acc: 0.9041 - val_loss: 0.5164 - val_acc: 0.8419\n",
      "Epoch 42/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2640 - acc: 0.9074 - val_loss: 0.5017 - val_acc: 0.8335\n",
      "Epoch 43/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2525 - acc: 0.9120 - val_loss: 0.5227 - val_acc: 0.8394\n",
      "Epoch 44/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2478 - acc: 0.9124 - val_loss: 0.5096 - val_acc: 0.8443\n",
      "Epoch 45/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2408 - acc: 0.9164 - val_loss: 0.5254 - val_acc: 0.8414\n",
      "Epoch 46/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2400 - acc: 0.9163 - val_loss: 0.5583 - val_acc: 0.8355\n",
      "Epoch 47/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2314 - acc: 0.9187 - val_loss: 0.4988 - val_acc: 0.8404\n",
      "Epoch 48/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2182 - acc: 0.9237 - val_loss: 0.5970 - val_acc: 0.8325\n",
      "Epoch 49/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2228 - acc: 0.9265 - val_loss: 0.5542 - val_acc: 0.8360\n",
      "Epoch 50/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2128 - acc: 0.9272 - val_loss: 0.5861 - val_acc: 0.8369\n",
      "Epoch 51/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2143 - acc: 0.9201 - val_loss: 0.6002 - val_acc: 0.8360\n",
      "Epoch 52/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2022 - acc: 0.9320 - val_loss: 0.5606 - val_acc: 0.8365\n",
      "Epoch 53/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2008 - acc: 0.9322 - val_loss: 0.6046 - val_acc: 0.8350\n",
      "Epoch 54/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.1933 - acc: 0.9308 - val_loss: 0.5793 - val_acc: 0.8365\n",
      "Epoch 55/100\n",
      "1170/8121 [===>..........................] - ETA: 7s - loss: 0.1834 - acc: 0.9333"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run(bs, lr, ks, num_layer):\n",
    "    \n",
    "    for index, (X_train, Y_train, X_val, Y_val, val_cat) in enumerate(zip(training_data,\n",
    "                                                       training_label,\n",
    "                                                       validation_data,\n",
    "                                                       validation_label,\n",
    "                                                       validation_cate_label)):\n",
    "        if index != 2: continue\n",
    "        model = tools.create_model(lr, bs, ks, num_layer)\n",
    "        print(\"Fold \"+str(index))\n",
    "        early_stop = EarlyStopping(patience=20)\n",
    "        history = model.fit(x = X_train, \n",
    "                            y = Y_train,\n",
    "                            epochs=100,\n",
    "                            validation_data=(X_val, Y_val),\n",
    "                            callbacks=[early_stop],\n",
    "                            batch_size=bs, \n",
    "                            verbose=1)\n",
    "        evaluation = model.evaluate(x = X_val, y = Y_val)\n",
    "        validation_prediction = model.predict_classes(X_val, batch_size=bs)\n",
    "        score = f1_score(val_cat, validation_prediction, average=None)\n",
    "        print(score)\n",
    "        \n",
    "#         tools.show_plot(inner_path, history)\n",
    "        \n",
    "        test_prediction = model.predict_classes(X_val, batch_size=1)\n",
    "        cnf_matrix = confusion_matrix(val_cat, test_prediction)\n",
    "        plot_confusion_matrix.plot_confusion_matrix(cnf_matrix, classes=['AF','Noise','Normal','Other'], save_png=True)\n",
    "        \n",
    "        return X_val, val_cat, validation_prediction\n",
    "    \n",
    "f3_X_val, f3val_cat, f3_validation_prediction = run(bs, lr, ks, num_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T14:02:22.498196Z",
     "start_time": "2019-06-19T13:55:43.742147Z"
    },
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3\n",
      "Train on 8121 samples, validate on 2030 samples\n",
      "Epoch 1/100\n",
      "8121/8121 [==============================] - 12s 1ms/step - loss: 1.0291 - acc: 0.5699 - val_loss: 0.9931 - val_acc: 0.5956\n",
      "Epoch 2/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.9294 - acc: 0.5881 - val_loss: 0.9313 - val_acc: 0.6453\n",
      "Epoch 3/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.8676 - acc: 0.6276 - val_loss: 0.7936 - val_acc: 0.6744\n",
      "Epoch 4/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.7945 - acc: 0.6625 - val_loss: 0.7161 - val_acc: 0.6961\n",
      "Epoch 5/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.7437 - acc: 0.6918 - val_loss: 0.7023 - val_acc: 0.6936\n",
      "Epoch 6/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.6891 - acc: 0.7136 - val_loss: 0.6046 - val_acc: 0.7463\n",
      "Epoch 7/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.6331 - acc: 0.7414 - val_loss: 0.5782 - val_acc: 0.7488\n",
      "Epoch 8/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.6002 - acc: 0.7542 - val_loss: 0.5559 - val_acc: 0.7739\n",
      "Epoch 9/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.5562 - acc: 0.7750 - val_loss: 0.5932 - val_acc: 0.7621\n",
      "Epoch 10/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.5424 - acc: 0.7902 - val_loss: 0.5206 - val_acc: 0.8064\n",
      "Epoch 11/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.5233 - acc: 0.8013 - val_loss: 0.5264 - val_acc: 0.8030\n",
      "Epoch 12/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.5062 - acc: 0.8107 - val_loss: 0.4845 - val_acc: 0.8158\n",
      "Epoch 13/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4873 - acc: 0.8154 - val_loss: 0.5124 - val_acc: 0.7931\n",
      "Epoch 14/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4728 - acc: 0.8245 - val_loss: 0.4931 - val_acc: 0.8094\n",
      "Epoch 15/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4649 - acc: 0.8285 - val_loss: 0.4848 - val_acc: 0.8113\n",
      "Epoch 16/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4433 - acc: 0.8357 - val_loss: 0.4803 - val_acc: 0.8153\n",
      "Epoch 17/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4358 - acc: 0.8373 - val_loss: 0.4917 - val_acc: 0.8054\n",
      "Epoch 18/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4340 - acc: 0.8391 - val_loss: 0.4694 - val_acc: 0.8256\n",
      "Epoch 19/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4289 - acc: 0.8445 - val_loss: 0.4497 - val_acc: 0.8340\n",
      "Epoch 20/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4185 - acc: 0.8439 - val_loss: 0.4305 - val_acc: 0.8345\n",
      "Epoch 21/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4098 - acc: 0.8504 - val_loss: 0.4645 - val_acc: 0.8212\n",
      "Epoch 22/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4052 - acc: 0.8492 - val_loss: 0.4397 - val_acc: 0.8374\n",
      "Epoch 23/100\n",
      " 330/8121 [>.............................] - ETA: 7s - loss: 0.4069 - acc: 0.8485"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3779 - acc: 0.8621 - val_loss: 0.4341 - val_acc: 0.8374\n",
      "Epoch 25/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3771 - acc: 0.8643 - val_loss: 0.4532 - val_acc: 0.8340\n",
      "Epoch 26/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3758 - acc: 0.8600 - val_loss: 0.4317 - val_acc: 0.8404\n",
      "Epoch 27/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3596 - acc: 0.8714 - val_loss: 0.4299 - val_acc: 0.8429\n",
      "Epoch 28/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3632 - acc: 0.8692 - val_loss: 0.4419 - val_acc: 0.8443\n",
      "Epoch 29/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3480 - acc: 0.8756 - val_loss: 0.4269 - val_acc: 0.8468\n",
      "Epoch 30/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2713 - acc: 0.9032 - val_loss: 0.4689 - val_acc: 0.8458\n",
      "Epoch 42/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2628 - acc: 0.9070 - val_loss: 0.4582 - val_acc: 0.8429\n",
      "Epoch 43/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2587 - acc: 0.9104 - val_loss: 0.4891 - val_acc: 0.8532\n",
      "2030/2030 [==============================] - 1s 336us/step\n",
      "[0.82674772 0.63793103 0.90692907 0.77040816]\n"
     ]
    }
   ],
   "source": [
    "def run(bs, lr, ks, num_layer):\n",
    "    \n",
    "    for index, (X_train, Y_train, X_val, Y_val, val_cat) in enumerate(zip(training_data,\n",
    "                                                       training_label,\n",
    "                                                       validation_data,\n",
    "                                                       validation_label,\n",
    "                                                       validation_cate_label)):\n",
    "        if index != 3: continue\n",
    "        model = tools.create_model(lr, bs, ks, num_layer)\n",
    "        print(\"Fold \"+str(index))\n",
    "        early_stop = EarlyStopping(patience=20)\n",
    "        history = model.fit(x = X_train, \n",
    "                            y = Y_train,\n",
    "                            epochs=100,\n",
    "                            validation_data=(X_val, Y_val),\n",
    "                            callbacks=[early_stop],\n",
    "                            batch_size=bs, \n",
    "                            verbose=1)\n",
    "        evaluation = model.evaluate(x = X_val, y = Y_val)\n",
    "        validation_prediction = model.predict_classes(X_val, batch_size=bs)\n",
    "        score = f1_score(val_cat, validation_prediction, average=None)\n",
    "        print(score)\n",
    "        \n",
    "#         tools.show_plot(inner_path, history)\n",
    "        \n",
    "        test_prediction = model.predict_classes(X_val, batch_size=1)\n",
    "        cnf_matrix = confusion_matrix(val_cat, test_prediction)\n",
    "        plot_confusion_matrix.plot_confusion_matrix(cnf_matrix, classes=['AF','Noise','Normal','Other'], save_png=True)\n",
    "        \n",
    "        return X_val, val_cat, validation_prediction\n",
    "    \n",
    "f4X_val, f4val_cat, f4_validation_prediction = run(bs, lr, ks, num_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T14:26:53.151423Z",
     "start_time": "2019-06-19T14:20:19.514087Z"
    },
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4\n",
      "Train on 8121 samples, validate on 2030 samples\n",
      "Epoch 1/100\n",
      "8121/8121 [==============================] - 12s 1ms/step - loss: 1.0146 - acc: 0.5801 - val_loss: 0.9534 - val_acc: 0.5773\n",
      "Epoch 2/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.9115 - acc: 0.5980 - val_loss: 0.8704 - val_acc: 0.6163\n",
      "Epoch 3/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.8610 - acc: 0.6339 - val_loss: 0.8319 - val_acc: 0.6483\n",
      "Epoch 4/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.7958 - acc: 0.6610 - val_loss: 0.7636 - val_acc: 0.6552\n",
      "Epoch 5/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.7503 - acc: 0.6861 - val_loss: 0.7567 - val_acc: 0.6621\n",
      "Epoch 6/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.6971 - acc: 0.7084 - val_loss: 0.6790 - val_acc: 0.7089\n",
      "Epoch 7/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.6664 - acc: 0.7291 - val_loss: 0.6955 - val_acc: 0.6990\n",
      "Epoch 8/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.5984 - acc: 0.7595 - val_loss: 0.6069 - val_acc: 0.7542\n",
      "Epoch 9/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.5756 - acc: 0.7676 - val_loss: 0.6039 - val_acc: 0.7567\n",
      "Epoch 10/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.5567 - acc: 0.7819 - val_loss: 0.5631 - val_acc: 0.7665\n",
      "Epoch 11/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.5243 - acc: 0.7949 - val_loss: 0.5468 - val_acc: 0.7788\n",
      "Epoch 12/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.5080 - acc: 0.8046 - val_loss: 0.5144 - val_acc: 0.7961\n",
      "Epoch 13/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4932 - acc: 0.8125 - val_loss: 0.5004 - val_acc: 0.8034\n",
      "Epoch 14/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4821 - acc: 0.8216 - val_loss: 0.5177 - val_acc: 0.7970\n",
      "Epoch 15/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4680 - acc: 0.8254 - val_loss: 0.5090 - val_acc: 0.7975\n",
      "Epoch 16/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4611 - acc: 0.8280 - val_loss: 0.4949 - val_acc: 0.8094\n",
      "Epoch 17/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4523 - acc: 0.8355 - val_loss: 0.5483 - val_acc: 0.8054\n",
      "Epoch 18/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4406 - acc: 0.8410 - val_loss: 0.4726 - val_acc: 0.8202\n",
      "Epoch 19/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4225 - acc: 0.8484 - val_loss: 0.4776 - val_acc: 0.8246\n",
      "Epoch 20/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4124 - acc: 0.8488 - val_loss: 0.4642 - val_acc: 0.8212\n",
      "Epoch 21/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4120 - acc: 0.8511 - val_loss: 0.4552 - val_acc: 0.8271\n",
      "Epoch 22/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.4071 - acc: 0.8540 - val_loss: 0.4587 - val_acc: 0.8232\n",
      "Epoch 23/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3882 - acc: 0.8613 - val_loss: 0.4390 - val_acc: 0.8350\n",
      "Epoch 24/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3783 - acc: 0.8650 - val_loss: 0.4586 - val_acc: 0.8296\n",
      "Epoch 25/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3718 - acc: 0.8671 - val_loss: 0.4678 - val_acc: 0.8271\n",
      "Epoch 26/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3671 - acc: 0.8694 - val_loss: 0.4515 - val_acc: 0.8335\n",
      "Epoch 27/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3646 - acc: 0.8684 - val_loss: 0.4762 - val_acc: 0.8227\n",
      "Epoch 28/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3584 - acc: 0.8765 - val_loss: 0.4649 - val_acc: 0.8369\n",
      "Epoch 29/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3480 - acc: 0.8761 - val_loss: 0.4446 - val_acc: 0.8369\n",
      "Epoch 30/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3379 - acc: 0.8825 - val_loss: 0.4581 - val_acc: 0.8261\n",
      "Epoch 31/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3360 - acc: 0.8810 - val_loss: 0.4699 - val_acc: 0.8281\n",
      "Epoch 32/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3256 - acc: 0.8852 - val_loss: 0.4894 - val_acc: 0.8325\n",
      "Epoch 33/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3204 - acc: 0.8871 - val_loss: 0.4623 - val_acc: 0.8414\n",
      "Epoch 34/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3144 - acc: 0.8862 - val_loss: 0.4713 - val_acc: 0.8360\n",
      "Epoch 35/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3063 - acc: 0.8923 - val_loss: 0.4670 - val_acc: 0.8345\n",
      "Epoch 36/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.3003 - acc: 0.8937 - val_loss: 0.4630 - val_acc: 0.8320\n",
      "Epoch 37/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2978 - acc: 0.8945 - val_loss: 0.4568 - val_acc: 0.8266\n",
      "Epoch 38/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2910 - acc: 0.8987 - val_loss: 0.4827 - val_acc: 0.8315\n",
      "Epoch 39/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2772 - acc: 0.9037 - val_loss: 0.4884 - val_acc: 0.8276\n",
      "Epoch 40/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2741 - acc: 0.9057 - val_loss: 0.4876 - val_acc: 0.8345\n",
      "Epoch 41/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2715 - acc: 0.9058 - val_loss: 0.4821 - val_acc: 0.8384\n",
      "Epoch 42/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2616 - acc: 0.9086 - val_loss: 0.4941 - val_acc: 0.8286\n",
      "Epoch 43/100\n",
      "8121/8121 [==============================] - 9s 1ms/step - loss: 0.2632 - acc: 0.9072 - val_loss: 0.4851 - val_acc: 0.8409\n",
      "2030/2030 [==============================] - 1s 340us/step\n",
      "[0.77675841 0.66206897 0.90029203 0.76070529]\n"
     ]
    }
   ],
   "source": [
    "def run(bs, lr, ks, num_layer):\n",
    "    \n",
    "    for index, (X_train, Y_train, X_val, Y_val, val_cat) in enumerate(zip(training_data,\n",
    "                                                       training_label,\n",
    "                                                       validation_data,\n",
    "                                                       validation_label,\n",
    "                                                       validation_cate_label)):\n",
    "        if index != 4: continue\n",
    "        model = tools.create_model(lr, bs, ks, num_layer)\n",
    "        print(\"Fold \"+str(index))\n",
    "        early_stop = EarlyStopping(patience=20)\n",
    "        history = model.fit(x = X_train, \n",
    "                            y = Y_train,\n",
    "                            epochs=100,\n",
    "                            validation_data=(X_val, Y_val),\n",
    "                            callbacks=[early_stop],\n",
    "                            batch_size=bs, \n",
    "                            verbose=1)\n",
    "        evaluation = model.evaluate(x = X_val, y = Y_val)\n",
    "        validation_prediction = model.predict_classes(X_val, batch_size=bs)\n",
    "        score = f1_score(val_cat, validation_prediction, average=None)\n",
    "        print(score)\n",
    "        \n",
    "#         tools.show_plot(inner_path, history)\n",
    "        \n",
    "        test_prediction = model.predict_classes(X_val, batch_size=1)\n",
    "        cnf_matrix = confusion_matrix(val_cat, test_prediction)\n",
    "        plot_confusion_matrix.plot_confusion_matrix(cnf_matrix, classes=['AF','Noise','Normal','Other'], save_png=True)\n",
    "        \n",
    "        return X_val, val_cat, validation_prediction\n",
    "    \n",
    "f5X_val, f5val_cat, f5_validation_prediction = run(bs, lr, ks, num_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
