{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/hsiehch/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: 8528\n",
      "label: 8528\n",
      "StratifiedKFold(n_splits=5, random_state=100, shuffle=True)\n",
      "trian: [   0    1    2 ... 8525 8526 8527] len 6821 test: [  10   12   19 ... 8515 8520 8524] len 1707\n",
      "trian: [   0    1    2 ... 8524 8525 8527] len 6822 test: [   3    9   15 ... 8509 8521 8526] len 1706\n",
      "trian: [   0    2    3 ... 8525 8526 8527] len 6822 test: [   1    5    6 ... 8513 8519 8523] len 1706\n",
      "trian: [   0    1    2 ... 8524 8526 8527] len 6823 test: [   8   11   26 ... 8517 8518 8525] len 1705\n",
      "trian: [   1    3    5 ... 8524 8525 8526] len 6824 test: [   0    2    4 ... 8512 8522 8527] len 1704\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import early_stopping\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import tools_with_GMP as tools\n",
    "import plot_confusion_matrix_Copy1 as plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "training_data, training_label, validation_data, validation_label, validation_cate_label = tools.get_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 606.,  223., 4060., 1932.], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(training_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(training_data, training_label):\n",
    "    n = 900\n",
    "    ii = 1\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "    for dat, lab in zip(training_data, training_label):\n",
    "\n",
    "        if ii <= 900 and (np.argmax(lab) == 2):\n",
    "            trainX.append(dat)\n",
    "            trainY.append(lab)\n",
    "            ii = ii + 1\n",
    "        if np.argmax(lab) != 2:\n",
    "            trainX.append(dat)\n",
    "            trainY.append(lab)\n",
    "    trainX = np.array(trainX)\n",
    "    trainY = np.array(trainY)\n",
    "    return trainX, trainY\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pylab as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "i=0\n",
    "\n",
    "scaler = StandardScaler()\n",
    "tt=scaler.fit_transform(training_data[0][i])\n",
    "\n",
    "print(training_label[0][i])\n",
    "# plt.plot(training_data[0][i])\n",
    "plt.plot(tt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, 1)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 64)          6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, None, 64)          12352     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 40,644\n",
      "Trainable params: 40,580\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch: 1\n",
      "  30/6821 [..............................] - ETA: 2:18 - train loss: 1.3831 - train accuracy: 0.4274"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0827 09:50:11.179541 139706075281152 deprecation.py:323] From /home/hsiehch/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1220: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6821/6821 [==============================] - 165s 24ms/step - train loss: 1.2829 - train accuracy: 0.4979\n",
      "1: 1.2829127131622065\n",
      "1707/1707 [==============================] - 199s 117ms/step - val loss: 1.1359 - val accuracy: 0.5958\n",
      "\n",
      "\n",
      "Epoch: 2\n",
      "6821/6821 [==============================] - 166s 24ms/step - train loss: 1.0983 - train accuracy: 0.5919\n",
      "2: 1.0982760400894422\n",
      "1707/1707 [==============================] - 201s 118ms/step - val loss: 1.0910 - val accuracy: 0.5958\n",
      "\n",
      "\n",
      "Epoch: 3\n",
      "6821/6821 [==============================] - 164s 24ms/step - train loss: 1.0631 - train accuracy: 0.5987\n",
      "3: 1.0631106566781139\n",
      "1707/1707 [==============================] - 198s 116ms/step - val loss: 1.0559 - val accuracy: 0.5958\n",
      "\n",
      "\n",
      "Epoch: 4\n",
      "6821/6821 [==============================] - 165s 24ms/step - train loss: 1.0506 - train accuracy: 0.5949\n",
      "4: 1.050601203740609\n",
      "1707/1707 [==============================] - 199s 116ms/step - val loss: 1.0453 - val accuracy: 0.5958\n",
      "\n",
      "\n",
      "Epoch: 5\n",
      "6821/6821 [==============================] - 184s 27ms/step - train loss: 1.0502 - train accuracy: 0.5976\n",
      "5: 1.050222153855828\n",
      "1707/1707 [==============================] - 258s 151ms/step - val loss: 1.0493 - val accuracy: 0.5958\n",
      "\n",
      "\n",
      "Epoch: 6\n",
      "6821/6821 [==============================] - 233s 34ms/step - train loss: 1.0265 - train accuracy: 0.5900\n",
      "6: 1.026497194624843\n",
      "1707/1707 [==============================] - 296s 174ms/step - val loss: 1.0228 - val accuracy: 0.5958\n",
      "\n",
      "\n",
      "Epoch: 7\n",
      "6821/6821 [==============================] - 369s 54ms/step - train loss: 1.0199 - train accuracy: 0.5977\n",
      "7: 1.0199168552898803\n",
      "1707/1707 [==============================] - 448s 263ms/step - val loss: 1.0201 - val accuracy: 0.5958\n",
      "\n",
      "\n",
      "Epoch: 8\n",
      "6821/6821 [==============================] - 372s 55ms/step - train loss: 1.0087 - train accuracy: 0.5942\n",
      "8: 1.0086621266174098\n",
      "1707/1707 [==============================] - 453s 266ms/step - val loss: 1.0098 - val accuracy: 0.5958\n",
      "\n",
      "\n",
      "Epoch: 9\n",
      "6821/6821 [==============================] - 377s 55ms/step - train loss: 1.0025 - train accuracy: 0.5925\n",
      "9: 1.0024707164629654\n",
      "1707/1707 [==============================] - 454s 266ms/step - val loss: 1.0043 - val accuracy: 0.5958\n",
      "\n",
      "\n",
      "Epoch: 10\n",
      "6821/6821 [==============================] - 349s 51ms/step - train loss: 1.0003 - train accuracy: 0.5906\n",
      "10: 1.0003445043937267\n",
      "1707/1707 [==============================] - 425s 249ms/step - val loss: 1.0016 - val accuracy: 0.5958\n",
      "\n",
      "\n",
      "Epoch: 11\n",
      "6821/6821 [==============================] - 351s 51ms/step - train loss: 0.9987 - train accuracy: 0.5899\n",
      "11: 0.9987062090461338\n",
      "1707/1707 [==============================] - 429s 251ms/step - val loss: 1.0053 - val accuracy: 0.5958\n",
      "\n",
      "\n",
      "Epoch: 12\n",
      "6821/6821 [==============================] - 369s 54ms/step - train loss: 1.0002 - train accuracy: 0.5930\n",
      "12: 1.0001752030771964\n",
      "1707/1707 [==============================] - 451s 264ms/step - val loss: 1.0004 - val accuracy: 0.5958\n",
      "\n",
      "\n",
      "Epoch: 13\n",
      "6821/6821 [==============================] - 374s 55ms/step - train loss: 0.9936 - train accuracy: 0.5893\n",
      "13: 0.9935507294394419\n",
      "1707/1707 [==============================] - 455s 267ms/step - val loss: 0.9921 - val accuracy: 0.5958\n",
      "\n",
      "\n",
      "Epoch: 14\n",
      "6821/6821 [==============================] - 337s 49ms/step - train loss: 0.9964 - train accuracy: 0.5821\n",
      "14: 0.9964223454463997\n",
      "1707/1707 [==============================] - 403s 236ms/step - val loss: 0.9930 - val accuracy: 0.5958\n",
      "\n",
      "\n",
      "Epoch: 15\n",
      "6821/6821 [==============================] - 306s 45ms/step - train loss: 0.9918 - train accuracy: 0.5906\n",
      "15: 0.991840155110924\n",
      "1707/1707 [==============================] - 372s 218ms/step - val loss: 0.9925 - val accuracy: 0.5958\n",
      "\n",
      "\n",
      "Epoch: 16\n",
      "6821/6821 [==============================] - 296s 43ms/step - train loss: 0.9913 - train accuracy: 0.5969\n",
      "16: 0.9912546751767045\n",
      "1707/1707 [==============================] - 330s 194ms/step - val loss: 0.9958 - val accuracy: 0.5958\n",
      "\n",
      "\n",
      "Epoch: 17\n",
      "6821/6821 [==============================] - 162s 24ms/step - train loss: 0.9909 - train accuracy: 0.5903\n",
      "17: 0.9909398504367041\n",
      "1707/1707 [==============================] - 197s 115ms/step - val loss: 0.9922 - val accuracy: 0.5958\n",
      "\n",
      "\n",
      "Epoch: 18\n",
      "6821/6821 [==============================] - 165s 24ms/step - train loss: 0.9869 - train accuracy: 0.5958\n",
      "18: 0.9869306431102733\n",
      "1707/1707 [==============================] - 200s 117ms/step - val loss: 0.9872 - val accuracy: 0.5958\n",
      "\n",
      "\n",
      "Epoch: 19\n",
      "6821/6821 [==============================] - 164s 24ms/step - train loss: 0.9842 - train accuracy: 0.5948\n",
      "19: 0.9841674752368305\n",
      "1707/1707 [==============================] - 199s 117ms/step - val loss: 0.9824 - val accuracy: 0.2766\n",
      "\n",
      "\n",
      "Epoch: 20\n",
      "6821/6821 [==============================] - 165s 24ms/step - train loss: 0.9843 - train accuracy: 0.5849\n",
      "20: 0.9843198035745628\n",
      "1707/1707 [==============================] - 200s 117ms/step - val loss: 0.9913 - val accuracy: 0.5958\n",
      "\n",
      "\n",
      "Epoch: 21\n",
      "6821/6821 [==============================] - 164s 24ms/step - train loss: 0.9832 - train accuracy: 0.5998\n",
      "21: 0.9832445727676105\n",
      "1707/1707 [==============================] - 199s 117ms/step - val loss: 0.9807 - val accuracy: 0.5958\n",
      "\n",
      "\n",
      "Epoch: 22\n",
      "6821/6821 [==============================] - 165s 24ms/step - train loss: 0.9917 - train accuracy: 0.5942\n",
      "22: 0.9916912536252581\n",
      "1707/1707 [==============================] - 200s 117ms/step - val loss: 0.9785 - val accuracy: 0.5958\n",
      "\n",
      "\n",
      "Early Stopping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.74623577 0.        ]\n",
      "\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, None, 1)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, None, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, None, 64)          6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, None, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, None, 64)          12352     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 40,644\n",
      "Trainable params: 40,580\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch: 1\n",
      "6822/6822 [==============================] - 158s 23ms/step - train loss: 1.2467 - train accuracy: 0.5309\n",
      "1: 1.2467357810839024\n",
      "1706/1706 [==============================] - 191s 112ms/step - val loss: 1.1073 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 2\n",
      "6822/6822 [==============================] - 158s 23ms/step - train loss: 1.0775 - train accuracy: 0.5915\n",
      "2: 1.0775245020472493\n",
      "1706/1706 [==============================] - 191s 112ms/step - val loss: 1.0523 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 3\n",
      "6822/6822 [==============================] - 159s 23ms/step - train loss: 1.0519 - train accuracy: 0.5928\n",
      "3: 1.0518870306334\n",
      "1706/1706 [==============================] - 192s 113ms/step - val loss: 1.0281 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 4\n",
      "6822/6822 [==============================] - 160s 23ms/step - train loss: 1.0364 - train accuracy: 0.6021\n",
      "4: 1.0364094652979234\n",
      "1706/1706 [==============================] - 194s 114ms/step - val loss: 1.0529 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 5\n",
      "6822/6822 [==============================] - 165s 24ms/step - train loss: 1.0173 - train accuracy: 0.5937\n",
      "5: 1.0172548156008745\n",
      "1706/1706 [==============================] - 201s 118ms/step - val loss: 1.0016 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 6\n",
      "6822/6822 [==============================] - 165s 24ms/step - train loss: 1.0046 - train accuracy: 0.5918\n",
      "6: 1.004576877671572\n",
      "1706/1706 [==============================] - 200s 117ms/step - val loss: 0.9945 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 7\n",
      "6822/6822 [==============================] - 165s 24ms/step - train loss: 1.0062 - train accuracy: 0.5940\n",
      "7: 1.006206531618649\n",
      "1706/1706 [==============================] - 200s 117ms/step - val loss: 1.0066 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 8\n",
      "6822/6822 [==============================] - 165s 24ms/step - train loss: 0.9939 - train accuracy: 0.5930\n",
      "8: 0.9939464491260556\n",
      "1706/1706 [==============================] - 201s 118ms/step - val loss: 0.9978 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 9\n",
      "6822/6822 [==============================] - 169s 25ms/step - train loss: 0.9925 - train accuracy: 0.5877\n",
      "9: 0.9924592437976533\n",
      "1706/1706 [==============================] - 206s 121ms/step - val loss: 0.9889 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 10\n",
      "6822/6822 [==============================] - 176s 26ms/step - train loss: 0.9907 - train accuracy: 0.5899\n",
      "10: 0.9907124764989558\n",
      "1706/1706 [==============================] - 214s 126ms/step - val loss: 0.9878 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Early Stopping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.74604925 0.        ]\n",
      "\n",
      "\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, None, 1)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, None, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, None, 64)          6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, None, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, None, 64)          12352     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 40,644\n",
      "Trainable params: 40,580\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch: 1\n",
      "6822/6822 [==============================] - 179s 26ms/step - train loss: 1.2989 - train accuracy: 0.4743\n",
      "1: 1.2989364545508322\n",
      "1706/1706 [==============================] - 217s 127ms/step - val loss: 1.1995 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 2\n",
      "6822/6822 [==============================] - 177s 26ms/step - train loss: 1.1086 - train accuracy: 0.5915\n",
      "2: 1.1086426438174437\n",
      "1706/1706 [==============================] - 214s 126ms/step - val loss: 1.1134 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 3\n",
      "6822/6822 [==============================] - 177s 26ms/step - train loss: 1.0785 - train accuracy: 0.5928\n",
      "3: 1.0785064832670668\n",
      "1706/1706 [==============================] - 215s 126ms/step - val loss: 1.0713 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 4\n",
      "6822/6822 [==============================] - 183s 27ms/step - train loss: 1.0523 - train accuracy: 0.6021\n",
      "4: 1.052308767781617\n",
      "1706/1706 [==============================] - 221s 130ms/step - val loss: 1.0515 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 5\n",
      "6822/6822 [==============================] - 173s 25ms/step - train loss: 1.0372 - train accuracy: 0.5937\n",
      "5: 1.0371967166278677\n",
      "1706/1706 [==============================] - 210s 123ms/step - val loss: 1.0350 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 6\n",
      "6822/6822 [==============================] - 174s 25ms/step - train loss: 1.0168 - train accuracy: 0.5918\n",
      "6: 1.0168382934830098\n",
      "1706/1706 [==============================] - 211s 124ms/step - val loss: 1.0100 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 7\n",
      "6822/6822 [==============================] - 178s 26ms/step - train loss: 1.0069 - train accuracy: 0.5940\n",
      "7: 1.0069270395866978\n",
      "1706/1706 [==============================] - 216s 127ms/step - val loss: 1.0083 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 8\n",
      "6822/6822 [==============================] - 178s 26ms/step - train loss: 0.9920 - train accuracy: 0.5930\n",
      "8: 0.9920158264280021\n",
      "1706/1706 [==============================] - 216s 127ms/step - val loss: 0.9992 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 9\n",
      "6822/6822 [==============================] - 177s 26ms/step - train loss: 0.9896 - train accuracy: 0.5877\n",
      "9: 0.9895646878398474\n",
      "1706/1706 [==============================] - 215s 126ms/step - val loss: 0.9851 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 10\n",
      "6822/6822 [==============================] - 179s 26ms/step - train loss: 0.9851 - train accuracy: 0.5899\n",
      "10: 0.9850682219778787\n",
      "1706/1706 [==============================] - 218s 128ms/step - val loss: 0.9872 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 11\n",
      "6822/6822 [==============================] - 178s 26ms/step - train loss: 0.9839 - train accuracy: 0.6021\n",
      "11: 0.9839038636105423\n",
      "1706/1706 [==============================] - 215s 126ms/step - val loss: 0.9856 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 12\n",
      "6822/6822 [==============================] - 178s 26ms/step - train loss: 0.9871 - train accuracy: 0.5890\n",
      "12: 0.9870711703344843\n",
      "1706/1706 [==============================] - 217s 127ms/step - val loss: 0.9900 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 13\n",
      "6822/6822 [==============================] - 185s 27ms/step - train loss: 0.9865 - train accuracy: 0.5927\n",
      "13: 0.9865096864155671\n",
      "1706/1706 [==============================] - 224s 131ms/step - val loss: 1.0095 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 14\n",
      "6822/6822 [==============================] - 190s 28ms/step - train loss: 0.9840 - train accuracy: 0.5961\n",
      "14: 0.984047045344083\n",
      "1706/1706 [==============================] - 229s 134ms/step - val loss: 0.9808 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 15\n",
      "6822/6822 [==============================] - 181s 27ms/step - train loss: 0.9801 - train accuracy: 0.5813\n",
      "15: 0.9800698781417606\n",
      "1706/1706 [==============================] - 219s 128ms/step - val loss: 0.9826 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 16\n",
      "6822/6822 [==============================] - 175s 26ms/step - train loss: 0.9864 - train accuracy: 0.5920\n",
      "16: 0.9864486528106277\n",
      "1706/1706 [==============================] - 214s 125ms/step - val loss: 0.9885 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 17\n",
      "6822/6822 [==============================] - 181s 26ms/step - train loss: 0.9865 - train accuracy: 0.5853\n",
      "17: 0.9864905604171822\n",
      "1706/1706 [==============================] - 218s 128ms/step - val loss: 1.0087 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 18\n",
      "6822/6822 [==============================] - 173s 25ms/step - train loss: 0.9840 - train accuracy: 0.6009\n",
      "18: 0.9840390954132547\n",
      "1706/1706 [==============================] - 211s 124ms/step - val loss: 0.9972 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 19\n",
      "6822/6822 [==============================] - 176s 26ms/step - train loss: 0.9799 - train accuracy: 0.5934\n",
      "19: 0.9799131534772733\n",
      "1706/1706 [==============================] - 213s 125ms/step - val loss: 0.9914 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 20\n",
      "6822/6822 [==============================] - 174s 25ms/step - train loss: 0.9767 - train accuracy: 0.6000\n",
      "20: 0.9767365313477295\n",
      "1706/1706 [==============================] - 212s 124ms/step - val loss: 0.9738 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 21\n",
      "6822/6822 [==============================] - 175s 26ms/step - train loss: 0.9735 - train accuracy: 0.5936\n",
      "21: 0.9735334849015227\n",
      "1706/1706 [==============================] - 212s 124ms/step - val loss: 0.9812 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 22\n",
      "6822/6822 [==============================] - 177s 26ms/step - train loss: 0.9711 - train accuracy: 0.5898\n",
      "22: 0.9711349698381262\n",
      "1706/1706 [==============================] - 216s 127ms/step - val loss: 0.9901 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 23\n",
      "6822/6822 [==============================] - 180s 26ms/step - train loss: 0.9774 - train accuracy: 0.5870\n",
      "23: 0.9774201529309049\n",
      "1706/1706 [==============================] - 217s 127ms/step - val loss: 0.9825 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Epoch: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6822/6822 [==============================] - 179s 26ms/step - train loss: 0.9753 - train accuracy: 0.6078\n",
      "24: 0.9752744861616395\n",
      "1706/1706 [==============================] - 217s 127ms/step - val loss: 0.9852 - val accuracy: 0.6094\n",
      "\n",
      "\n",
      "Early Stopping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.74604925 0.        ]\n",
      "\n",
      "\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, None, 1)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, None, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, None, 64)          6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, None, 64)          12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, None, 64)          12352     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 40,644\n",
      "Trainable params: 40,580\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch: 1\n",
      "6823/6823 [==============================] - 169s 25ms/step - train loss: 1.2976 - train accuracy: 0.4807\n",
      "1: 1.2975967615865893\n",
      "1705/1705 [==============================] - 206s 121ms/step - val loss: 1.2297 - val accuracy: 0.6097\n",
      "\n",
      "\n",
      "Epoch: 2\n",
      "6823/6823 [==============================] - 173s 25ms/step - train loss: 1.1555 - train accuracy: 0.5940\n",
      "2: 1.1555158512649126\n",
      "1705/1705 [==============================] - 211s 124ms/step - val loss: 1.1043 - val accuracy: 0.6097\n",
      "\n",
      "\n",
      "Epoch: 3\n",
      "6823/6823 [==============================] - 175s 26ms/step - train loss: 1.0766 - train accuracy: 0.5877\n",
      "3: 1.0766001122322257\n",
      "1705/1705 [==============================] - 213s 125ms/step - val loss: 1.0474 - val accuracy: 0.6097\n",
      "\n",
      "\n",
      "Epoch: 4\n",
      "6823/6823 [==============================] - 177s 26ms/step - train loss: 1.0372 - train accuracy: 0.5965\n",
      "4: 1.0371999619366392\n",
      "1705/1705 [==============================] - 215s 126ms/step - val loss: 1.0313 - val accuracy: 0.6097\n",
      "\n",
      "\n",
      "Epoch: 5\n",
      "6823/6823 [==============================] - 173s 25ms/step - train loss: 1.0172 - train accuracy: 0.5853\n",
      "5: 1.017190301126618\n",
      "1705/1705 [==============================] - 210s 123ms/step - val loss: 1.0112 - val accuracy: 0.6097\n",
      "\n",
      "\n",
      "Epoch: 6\n",
      "6823/6823 [==============================] - 172s 25ms/step - train loss: 1.0025 - train accuracy: 0.6005\n",
      "6: 1.0025133404981537\n",
      "1705/1705 [==============================] - 209s 123ms/step - val loss: 0.9966 - val accuracy: 0.6097\n",
      "\n",
      "\n",
      "Epoch: 7\n",
      "6823/6823 [==============================] - 326s 48ms/step - train loss: 0.9957 - train accuracy: 0.5918\n",
      "7: 0.9956840507643222\n",
      "1705/1705 [==============================] - 365s 214ms/step - val loss: 1.0027 - val accuracy: 0.6097\n",
      "\n",
      "\n",
      "Epoch: 8\n",
      "6823/6823 [==============================] - 354s 52ms/step - train loss: 0.9915 - train accuracy: 0.5861\n",
      "8: 0.9914983849648837\n",
      "1705/1705 [==============================] - 426s 250ms/step - val loss: 0.9909 - val accuracy: 0.6097\n",
      "\n",
      "\n",
      "Epoch: 9\n",
      "6823/6823 [==============================] - 368s 54ms/step - train loss: 0.9932 - train accuracy: 0.6043\n",
      "9: 0.9931679764844031\n",
      "1705/1705 [==============================] - 443s 260ms/step - val loss: 0.9826 - val accuracy: 0.6097\n",
      "\n",
      "\n",
      "Epoch: 10\n",
      "3684/6823 [===============>..............] - ETA: 2:24 - train loss: 1.0002 - train accuracy: 0.5956"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bb7bf99c41dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-bb7bf99c41dc>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(bs, lr, ks, num_layer)\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0minput_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                     \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0mepoch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    711\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 712\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    751\u001b[0m                                 ' implement a `call` method.')\n\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    711\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 712\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'causal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_causal_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    205\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NCHW'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NHWC'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m   2657\u001b[0m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m       \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bias\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_nn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"BiasAdd\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m         data_format)\n\u001b[0m\u001b[1;32m    743\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras_radam.optimizer_v2 import RAdam\n",
    "ks = 3\n",
    "num_layer = 5\n",
    "bs = 30\n",
    "lr = 0.0001\n",
    "epochs = 100\n",
    "scores = []\n",
    "\n",
    "def create_model(learning_rate, bs, ks, num_layer):\n",
    "    num_filter = 32\n",
    "    \n",
    "    in_data = tf.keras.Input(shape=(None, 1), dtype=\"float64\")\n",
    "    x = tf.keras.layers.Conv1D(filters = num_filter, kernel_size = ks, activation=\"relu\")(in_data)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPool1D(2)(x)\n",
    "\n",
    "    for i in range(2,num_layer+1):\n",
    "        try:\n",
    "            if i==num_layer:\n",
    "                x = tf.keras.layers.Conv1D(filters = num_filter, kernel_size = ks, activation=\"relu\")(x)\n",
    "                break\n",
    "            if i%2 != 0:\n",
    "                num_filter = num_filter *2\n",
    "            x = tf.keras.layers.Conv1D(filters = num_filter, kernel_size = ks, activation=\"relu\")(x)\n",
    "            x = tf.keras.layers.MaxPool1D(2)(x)\n",
    "            if i in [6, 8, 9]:\n",
    "                x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        except ValueError:\n",
    "            print(\"model overflow[lr, bs, ks, #layer]: \",[learning_rate, bs, ks, i+1])\n",
    "            return False\n",
    "    \n",
    "    x = tf.keras.layers.GlobalMaxPool1D()(x)\n",
    "    x = tf.keras.layers.Dense(64, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Dense(32, activation = 'relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(4, activation = 'softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=in_data, outputs=outputs)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def run(bs, lr, ks, num_layer):\n",
    "    fold=1\n",
    "    for index, (X_train, Y_train, X_val, Y_val, val_cat) in enumerate(zip(training_data,\n",
    "                                                       training_label,\n",
    "                                                       validation_data,\n",
    "                                                       validation_label,\n",
    "                                                       validation_cate_label)):\n",
    "\n",
    "                \n",
    "        X_val, Y_val, val_cat = shuffle(X_val, Y_val, val_cat, random_state=50)\n",
    "        ES = early_stopping.EarlyStopping(patience=6)\n",
    "        model = create_model(lr, bs, ks, num_layer)\n",
    "        optimizer = tf.keras.optimizers.Adam(lr = lr)\n",
    "#         optimizer = tf.keras.optimizers.SGD()\n",
    "#         optimizer = RAdam(learning_rate=lr)\n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(1, epochs+1, 1):\n",
    "            print(\"Epoch: {}\".format(epoch))\n",
    "            epoch_losses = []\n",
    "            \n",
    "            prog_bar = tf.keras.utils.Progbar(X_train.shape[0])\n",
    "            prog_bar_val = tf.keras.utils.Progbar(X_val.shape[0])\n",
    "            train_acc_metric = tf.keras.metrics.Accuracy()\n",
    "            val_acc_metric = tf.keras.metrics.Accuracy()\n",
    "            \n",
    "            X_train, Y_train = shuffle(X_train, Y_train, random_state=0)\n",
    "            \n",
    "            for ind, (input_data, input_label) in enumerate(zip(X_train, Y_train)):\n",
    "                \n",
    "                with tf.GradientTape() as tape:\n",
    "                    input_data = input_data.reshape((1, input_data.shape[0], 1))\n",
    "                    input_label = input_label.reshape((1, input_label.shape[0]))\n",
    "                    logits = model(input_data)\n",
    "                    loss_value = tf.keras.losses.categorical_crossentropy(input_label, logits)\n",
    "                    epoch_losses.append(float(loss_value))\n",
    "                \n",
    "                train_acc_metric.update_state(np.argmax(input_label), np.argmax(logits))\n",
    "                prog_bar.add(1, values=[(\"train loss\", float(loss_value)), (\"train accuracy\", float(train_acc_metric.result()))])\n",
    "                \n",
    "                # update weights using mini-batch mechanism\n",
    "                if (ind+1)%bs == 0:\n",
    "                    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "                    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "            \n",
    "            \n",
    "            # weights update for the last batch\n",
    "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "            \n",
    "            avg_epoch_loss = sum(epoch_losses) / (1.0 * len(epoch_losses))\n",
    "            print(\"{}: {}\".format(epoch, avg_epoch_loss))\n",
    "            losses.append(avg_epoch_loss)\n",
    "            \n",
    "            for ind, (input_data, input_label) in enumerate(zip(X_val, Y_val)):\n",
    "                input_data = input_data.reshape((1, input_data.shape[0], 1))\n",
    "                input_label = input_label.reshape((1, input_label.shape[0]))\n",
    "                val_logits = model(input_data)\n",
    "                val_loss_val = tf.keras.losses.categorical_crossentropy(input_label, val_logits)\n",
    "                val_acc_metric.update_state(np.argmax(input_label), np.argmax(logits))\n",
    "                prog_bar_val.add(1, values=[(\"val loss\", float(val_loss_val)), (\"val accuracy\", float(val_acc_metric.result()))])\n",
    "        \n",
    "            print(\"\\n\")\n",
    "            \n",
    "            ES(float(val_loss_val), model, fold)\n",
    "            if ES.early_stop:\n",
    "                print(\"Early Stopping!\")\n",
    "                break\n",
    "        \n",
    "        val_pred_cat = []\n",
    "        for ind, (input_data, input_label) in enumerate(zip(X_val, Y_val)):\n",
    "            input_data = input_data.reshape((1, input_data.shape[0], 1))\n",
    "            input_label = input_label.reshape((1, input_label.shape[0]))\n",
    "            val_pred = model.predict(input_data)\n",
    "            val_pred_cat.append(np.argmax(val_pred))\n",
    "        \n",
    "        score = f1_score(val_cat, val_pred_cat, average=None)\n",
    "        scores.append(score)\n",
    "        print(score)\n",
    "        \n",
    "        fold = fold + 1\n",
    "        \n",
    "#         cnf_matrix = confusion_matrix(val_cat, val_pred_cat)\n",
    "#         plot_confusion_matrix.plot_confusion_matrix(cnf_matrix, classes=['AF','Noise','Normal','Other'], save_png=True)\n",
    "        \n",
    "        print(\"\\n\")\n",
    "\n",
    "run(bs, lr, ks, num_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
