{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 903\n",
      "~: 299\n",
      "O: 2990\n",
      "N: 5959\n",
      "split train data index:  [[0, 633, 904, 903], [903, 1113, 1203, 1202], [1202, 3296, 4194, 4192], [4192, 8364, 10152, 10151]]\n",
      "(7109, 9000)\n",
      "(7109,)\n",
      "(3042, 9000)\n",
      "(3042,)\n",
      "(0,)\n",
      "(0,)\n",
      "Train Data: (7109, 9000, 1)\n",
      "Train Label:  (7109, 4)\n",
      "Train Data: (3042, 9000, 1)\n",
      "Train Label:  (3042, 4)\n",
      "Index 1\n",
      "Train on 7109 samples, validate on 3042 samples\n",
      "Epoch 1/80\n",
      "7109/7109 [==============================] - 20s 3ms/step - loss: 1.0359 - acc: 0.5818 - val_loss: 1.0223 - val_acc: 0.5874\n",
      "Epoch 2/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.9776 - acc: 0.5853 - val_loss: 0.9492 - val_acc: 0.5917\n",
      "Epoch 3/80\n",
      "7109/7109 [==============================] - 10s 1ms/step - loss: 0.9394 - acc: 0.5883 - val_loss: 0.8859 - val_acc: 0.5927\n",
      "Epoch 4/80\n",
      "7109/7109 [==============================] - 10s 1ms/step - loss: 0.8876 - acc: 0.5936 - val_loss: 0.8621 - val_acc: 0.5960\n",
      "Epoch 5/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.8378 - acc: 0.6167 - val_loss: 0.7996 - val_acc: 0.6502\n",
      "Epoch 6/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.7959 - acc: 0.6607 - val_loss: 0.7435 - val_acc: 0.6716\n",
      "Epoch 7/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.7412 - acc: 0.6805 - val_loss: 0.7019 - val_acc: 0.6913\n",
      "Epoch 8/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.6981 - acc: 0.7083 - val_loss: 0.6497 - val_acc: 0.7156\n",
      "Epoch 9/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.6520 - acc: 0.7294 - val_loss: 0.6506 - val_acc: 0.7166\n",
      "Epoch 10/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.6149 - acc: 0.7433 - val_loss: 0.6179 - val_acc: 0.7331\n",
      "Epoch 11/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.5805 - acc: 0.7635 - val_loss: 0.5757 - val_acc: 0.7551\n",
      "Epoch 12/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.5496 - acc: 0.7863 - val_loss: 0.5083 - val_acc: 0.7991\n",
      "Epoch 13/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.5231 - acc: 0.7983 - val_loss: 0.5255 - val_acc: 0.7945\n",
      "Epoch 14/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.5052 - acc: 0.8084 - val_loss: 0.5442 - val_acc: 0.7847\n",
      "Epoch 15/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.4944 - acc: 0.8154 - val_loss: 0.4887 - val_acc: 0.8133\n",
      "Epoch 16/80\n",
      "7109/7109 [==============================] - 10s 1ms/step - loss: 0.4661 - acc: 0.8252 - val_loss: 0.4985 - val_acc: 0.8044\n",
      "Epoch 17/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.4685 - acc: 0.8261 - val_loss: 0.4871 - val_acc: 0.8159\n",
      "Epoch 18/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.4505 - acc: 0.8312 - val_loss: 0.4968 - val_acc: 0.8011\n",
      "Epoch 19/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.4483 - acc: 0.8319 - val_loss: 0.4671 - val_acc: 0.8245\n",
      "Epoch 20/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.4317 - acc: 0.8429 - val_loss: 0.5052 - val_acc: 0.8067\n",
      "Epoch 21/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.4339 - acc: 0.8413 - val_loss: 0.4827 - val_acc: 0.8093\n",
      "Epoch 22/80\n",
      "7109/7109 [==============================] - 10s 1ms/step - loss: 0.4238 - acc: 0.8471 - val_loss: 0.4718 - val_acc: 0.8245\n",
      "Epoch 23/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.4130 - acc: 0.8458 - val_loss: 0.4661 - val_acc: 0.8172\n",
      "Epoch 24/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.4142 - acc: 0.8467 - val_loss: 0.4579 - val_acc: 0.8314\n",
      "Epoch 25/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.4080 - acc: 0.8498 - val_loss: 0.4691 - val_acc: 0.8271\n",
      "Epoch 26/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.3967 - acc: 0.8572 - val_loss: 0.4703 - val_acc: 0.8192\n",
      "Epoch 27/80\n",
      "7109/7109 [==============================] - 10s 1ms/step - loss: 0.3914 - acc: 0.8591 - val_loss: 0.4704 - val_acc: 0.8333\n",
      "Epoch 28/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.3870 - acc: 0.8582 - val_loss: 0.4705 - val_acc: 0.8284\n",
      "Epoch 29/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.3765 - acc: 0.8638 - val_loss: 0.4653 - val_acc: 0.8228\n",
      "Epoch 30/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.3673 - acc: 0.8671 - val_loss: 0.4762 - val_acc: 0.8218\n",
      "Epoch 31/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.3632 - acc: 0.8700 - val_loss: 0.4774 - val_acc: 0.8241\n",
      "Epoch 32/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.3620 - acc: 0.8695 - val_loss: 0.4718 - val_acc: 0.8218\n",
      "Epoch 33/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.3603 - acc: 0.8714 - val_loss: 0.4809 - val_acc: 0.8304\n",
      "Epoch 34/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.3557 - acc: 0.8717 - val_loss: 0.4883 - val_acc: 0.8297\n",
      "Epoch 35/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.3493 - acc: 0.8766 - val_loss: 0.4679 - val_acc: 0.8323\n",
      "Epoch 36/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.3449 - acc: 0.8783 - val_loss: 0.4592 - val_acc: 0.8307\n",
      "Epoch 37/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.3341 - acc: 0.8827 - val_loss: 0.4684 - val_acc: 0.8340\n",
      "Epoch 38/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.3325 - acc: 0.8824 - val_loss: 0.4720 - val_acc: 0.8294\n",
      "Epoch 39/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.3224 - acc: 0.8848 - val_loss: 0.5152 - val_acc: 0.8314\n",
      "Epoch 40/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.3105 - acc: 0.8880 - val_loss: 0.5054 - val_acc: 0.8248\n",
      "Epoch 41/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.3121 - acc: 0.8928 - val_loss: 0.4862 - val_acc: 0.8297\n",
      "Epoch 42/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.3023 - acc: 0.8924 - val_loss: 0.5005 - val_acc: 0.8277\n",
      "Epoch 43/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.3036 - acc: 0.8915 - val_loss: 0.5399 - val_acc: 0.8254\n",
      "Epoch 44/80\n",
      "7109/7109 [==============================] - 9s 1ms/step - loss: 0.2948 - acc: 0.8948 - val_loss: 0.5254 - val_acc: 0.8314\n",
      "3042/3042 [==============================] - 1s 424us/step\n",
      "[0.77205882 0.64814815 0.89687586 0.73462214]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import itertools\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import f1_score\n",
    "import tools_split_70_30 as tools\n",
    "\n",
    "training_data, training_label, validation_data, validation_label, validation_cate_label = tools.get_data()\n",
    "\n",
    "kernel_size = [5]\n",
    "num_layers = [10]\n",
    "batch_size = [30]\n",
    "learning_rate = [0.0001]\n",
    "overflow_model = 0\n",
    "\n",
    "def run(bs, lr, ks, num_layer):\n",
    "    model = tools.create_model(lr, bs, ks, num_layer)\n",
    "    early_stop = EarlyStopping(patience=20)\n",
    "    history = model.fit(x = training_data, \n",
    "                        y = training_label,\n",
    "                        epochs=80,\n",
    "                        validation_data=(validation_data, validation_label),\n",
    "                        callbacks=[early_stop],\n",
    "                        batch_size=bs, \n",
    "                        verbose=1)\n",
    "    evaluation = model.evaluate(x = validation_data, y = validation_label)\n",
    "    validation_prediction = model.predict_classes(validation_data, batch_size=bs)\n",
    "    score = f1_score(validation_cate_label, validation_prediction, average=None)\n",
    "    \n",
    "    del model\n",
    "    return score\n",
    "\n",
    "## main()\n",
    "\n",
    "index=1\n",
    "for ks, num_layer, bs, lr in itertools.product(kernel_size, num_layers, batch_size, learning_rate):\n",
    "    print(\"Index \"+str(index))\n",
    "    \n",
    "    model = tools.create_model(lr, bs, ks, num_layer)\n",
    "    \n",
    "    score = run(bs, lr, ks, num_layer)\n",
    "    print(score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 903\n",
      "~: 299\n",
      "O: 2990\n",
      "N: 5959\n",
      "split train data index:  [[0, 723, 904, 903], [903, 1143, 1203, 1202], [1202, 3595, 4194, 4192], [4192, 8960, 10152, 10151]]\n",
      "(8124, 9000)\n",
      "(8124,)\n",
      "(2027, 9000)\n",
      "(2027,)\n",
      "(0,)\n",
      "(0,)\n",
      "Train Data: (8124, 9000, 1)\n",
      "Train Label:  (8124, 4)\n",
      "Train Data: (2027, 9000, 1)\n",
      "Train Label:  (2027, 4)\n",
      "Index 1\n",
      "Train on 8124 samples, validate on 2027 samples\n",
      "Epoch 1/80\n",
      "8124/8124 [==============================] - 19s 2ms/step - loss: 1.0272 - acc: 0.5774 - val_loss: 0.9756 - val_acc: 0.5876\n",
      "Epoch 2/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.9663 - acc: 0.5865 - val_loss: 0.9145 - val_acc: 0.5955\n",
      "Epoch 3/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.9012 - acc: 0.5953 - val_loss: 0.8823 - val_acc: 0.6201\n",
      "Epoch 4/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.8537 - acc: 0.6124 - val_loss: 0.8293 - val_acc: 0.6275\n",
      "Epoch 5/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.8137 - acc: 0.6512 - val_loss: 0.7689 - val_acc: 0.6774\n",
      "Epoch 6/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.7585 - acc: 0.6777 - val_loss: 0.7317 - val_acc: 0.6941\n",
      "Epoch 7/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.6984 - acc: 0.7085 - val_loss: 0.6533 - val_acc: 0.7178\n",
      "Epoch 8/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.6532 - acc: 0.7272 - val_loss: 0.6152 - val_acc: 0.7400\n",
      "Epoch 9/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.6212 - acc: 0.7402 - val_loss: 0.5993 - val_acc: 0.7489\n",
      "Epoch 10/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.5866 - acc: 0.7616 - val_loss: 0.5681 - val_acc: 0.7760\n",
      "Epoch 11/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.5640 - acc: 0.7763 - val_loss: 0.5347 - val_acc: 0.7864\n",
      "Epoch 12/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.5307 - acc: 0.7996 - val_loss: 0.5108 - val_acc: 0.8017\n",
      "Epoch 13/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.5091 - acc: 0.8111 - val_loss: 0.5072 - val_acc: 0.8032\n",
      "Epoch 14/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.4969 - acc: 0.8135 - val_loss: 0.5119 - val_acc: 0.8061\n",
      "Epoch 15/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.4885 - acc: 0.8197 - val_loss: 0.5126 - val_acc: 0.8032\n",
      "Epoch 16/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.4696 - acc: 0.8278 - val_loss: 0.5189 - val_acc: 0.7928\n",
      "Epoch 17/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.4620 - acc: 0.8261 - val_loss: 0.4771 - val_acc: 0.8175\n",
      "Epoch 18/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.4583 - acc: 0.8339 - val_loss: 0.4667 - val_acc: 0.8263\n",
      "Epoch 19/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.4417 - acc: 0.8386 - val_loss: 0.4829 - val_acc: 0.8170\n",
      "Epoch 20/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.4322 - acc: 0.8408 - val_loss: 0.4628 - val_acc: 0.8204\n",
      "Epoch 21/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.4342 - acc: 0.8406 - val_loss: 0.4517 - val_acc: 0.8333\n",
      "Epoch 22/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.4211 - acc: 0.8452 - val_loss: 0.4365 - val_acc: 0.8392\n",
      "Epoch 23/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.4102 - acc: 0.8530 - val_loss: 0.4541 - val_acc: 0.8244\n",
      "Epoch 24/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.4098 - acc: 0.8487 - val_loss: 0.4570 - val_acc: 0.8234\n",
      "Epoch 25/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.4002 - acc: 0.8519 - val_loss: 0.4634 - val_acc: 0.8288\n",
      "Epoch 26/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.3997 - acc: 0.8544 - val_loss: 0.4777 - val_acc: 0.8337\n",
      "Epoch 27/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.3866 - acc: 0.8567 - val_loss: 0.4547 - val_acc: 0.8318\n",
      "Epoch 28/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.3774 - acc: 0.8653 - val_loss: 0.4721 - val_acc: 0.8106\n",
      "Epoch 29/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.3720 - acc: 0.8687 - val_loss: 0.4739 - val_acc: 0.8244\n",
      "Epoch 30/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.3696 - acc: 0.8705 - val_loss: 0.4625 - val_acc: 0.8303\n",
      "Epoch 31/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.3667 - acc: 0.8664 - val_loss: 0.4455 - val_acc: 0.8362\n",
      "Epoch 32/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.3580 - acc: 0.8699 - val_loss: 0.4421 - val_acc: 0.8298\n",
      "Epoch 33/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.3534 - acc: 0.8728 - val_loss: 0.4364 - val_acc: 0.8397\n",
      "Epoch 34/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.3497 - acc: 0.8753 - val_loss: 0.4420 - val_acc: 0.8387\n",
      "Epoch 35/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.3431 - acc: 0.8752 - val_loss: 0.4777 - val_acc: 0.8185\n",
      "Epoch 36/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.3412 - acc: 0.8788 - val_loss: 0.4892 - val_acc: 0.8382\n",
      "Epoch 37/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.3308 - acc: 0.8822 - val_loss: 0.4814 - val_acc: 0.8254\n",
      "Epoch 38/80\n",
      "8124/8124 [==============================] - 9s 1ms/step - loss: 0.3140 - acc: 0.8924 - val_loss: 0.4900 - val_acc: 0.8234\n",
      "Epoch 39/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.3221 - acc: 0.8831 - val_loss: 0.4704 - val_acc: 0.8234\n",
      "Epoch 40/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.3167 - acc: 0.8864 - val_loss: 0.4652 - val_acc: 0.8308\n",
      "Epoch 41/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.3109 - acc: 0.8907 - val_loss: 0.4592 - val_acc: 0.8362\n",
      "Epoch 42/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.3078 - acc: 0.8911 - val_loss: 0.4536 - val_acc: 0.8372\n",
      "Epoch 43/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.3011 - acc: 0.8945 - val_loss: 0.4928 - val_acc: 0.8259\n",
      "Epoch 44/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.2984 - acc: 0.8946 - val_loss: 0.4457 - val_acc: 0.8431\n",
      "Epoch 45/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.2847 - acc: 0.9008 - val_loss: 0.4700 - val_acc: 0.8372\n",
      "Epoch 46/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.2802 - acc: 0.9014 - val_loss: 0.5387 - val_acc: 0.8273\n",
      "Epoch 47/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.2766 - acc: 0.9029 - val_loss: 0.4851 - val_acc: 0.8283\n",
      "Epoch 48/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.2791 - acc: 0.9029 - val_loss: 0.4766 - val_acc: 0.8352\n",
      "Epoch 49/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.2697 - acc: 0.9056 - val_loss: 0.4829 - val_acc: 0.8249\n",
      "Epoch 50/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.2619 - acc: 0.9085 - val_loss: 0.4921 - val_acc: 0.8293\n",
      "Epoch 51/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.2629 - acc: 0.9092 - val_loss: 0.4978 - val_acc: 0.8288\n",
      "Epoch 52/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.2607 - acc: 0.9088 - val_loss: 0.5376 - val_acc: 0.8357\n",
      "Epoch 53/80\n",
      "8124/8124 [==============================] - 10s 1ms/step - loss: 0.2503 - acc: 0.9117 - val_loss: 0.5146 - val_acc: 0.8175\n",
      "2027/2027 [==============================] - 1s 440us/step\n",
      "[0.74441687 0.71428571 0.88364105 0.72408027]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import itertools\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import f1_score\n",
    "import tools_split_80_20 as tools\n",
    "\n",
    "training_data, training_label, validation_data, validation_label, validation_cate_label = tools.get_data()\n",
    "\n",
    "kernel_size = [5]\n",
    "num_layers = [10]\n",
    "batch_size = [30]\n",
    "learning_rate = [0.0001]\n",
    "overflow_model = 0\n",
    "\n",
    "def run(bs, lr, ks, num_layer):\n",
    "    model = tools.create_model(lr, bs, ks, num_layer)\n",
    "    early_stop = EarlyStopping(patience=20)\n",
    "    history = model.fit(x = training_data, \n",
    "                        y = training_label,\n",
    "                        epochs=80,\n",
    "                        validation_data=(validation_data, validation_label),\n",
    "                        callbacks=[early_stop],\n",
    "                        batch_size=bs, \n",
    "                        verbose=1)\n",
    "    evaluation = model.evaluate(x = validation_data, y = validation_label)\n",
    "    validation_prediction = model.predict_classes(validation_data, batch_size=bs)\n",
    "    score = f1_score(validation_cate_label, validation_prediction, average=None)\n",
    "    \n",
    "    del model\n",
    "    return score\n",
    "\n",
    "## main()\n",
    "index=1\n",
    "for ks, num_layer, bs, lr in itertools.product(kernel_size, num_layers, batch_size, learning_rate):\n",
    "    print(\"Index \"+str(index))\n",
    "    \n",
    "    model = tools.create_model(lr, bs, ks, num_layer)\n",
    "    \n",
    "    score = run(bs, lr, ks, num_layer)\n",
    "    print(score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 60:40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 903\n",
      "~: 299\n",
      "O: 2990\n",
      "N: 5959\n",
      "split train data index:  [[0, 542, 904, 903], [903, 1083, 1203, 1202], [1202, 2997, 4194, 4192], [4192, 7768, 10152, 10151]]\n",
      "(6093, 9000)\n",
      "(6093,)\n",
      "(4058, 9000)\n",
      "(4058,)\n",
      "(0,)\n",
      "(0,)\n",
      "Train Data: (6093, 9000, 1)\n",
      "Train Label:  (6093, 4)\n",
      "Train Data: (4058, 9000, 1)\n",
      "Train Label:  (4058, 4)\n",
      "Index 1\n",
      "Train on 6093 samples, validate on 4058 samples\n",
      "Epoch 1/80\n",
      "6093/6093 [==============================] - 19s 3ms/step - loss: 1.0679 - acc: 0.5629 - val_loss: 1.0351 - val_acc: 0.5872\n",
      "Epoch 2/80\n",
      "6093/6093 [==============================] - 9s 1ms/step - loss: 0.9950 - acc: 0.5821 - val_loss: 1.0131 - val_acc: 0.5877\n",
      "Epoch 3/80\n",
      "6093/6093 [==============================] - 9s 1ms/step - loss: 0.9571 - acc: 0.5869 - val_loss: 0.9803 - val_acc: 0.5956\n",
      "Epoch 4/80\n",
      "6093/6093 [==============================] - 9s 1ms/step - loss: 0.9109 - acc: 0.5986 - val_loss: 0.8994 - val_acc: 0.6134\n",
      "Epoch 5/80\n",
      "6093/6093 [==============================] - 9s 1ms/step - loss: 0.8714 - acc: 0.6043 - val_loss: 0.9027 - val_acc: 0.6471\n",
      "Epoch 6/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.8334 - acc: 0.6427 - val_loss: 0.7846 - val_acc: 0.6626\n",
      "Epoch 7/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.7571 - acc: 0.6783 - val_loss: 0.7181 - val_acc: 0.6870\n",
      "Epoch 8/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.7196 - acc: 0.6998 - val_loss: 0.6679 - val_acc: 0.7095\n",
      "Epoch 9/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.6789 - acc: 0.7162 - val_loss: 0.6649 - val_acc: 0.7385\n",
      "Epoch 10/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.6422 - acc: 0.7366 - val_loss: 0.6533 - val_acc: 0.7169\n",
      "Epoch 11/80\n",
      "6093/6093 [==============================] - 9s 1ms/step - loss: 0.6227 - acc: 0.7441 - val_loss: 0.6033 - val_acc: 0.7361\n",
      "Epoch 12/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.5796 - acc: 0.7681 - val_loss: 0.5795 - val_acc: 0.7511\n",
      "Epoch 13/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.5507 - acc: 0.7825 - val_loss: 0.7055 - val_acc: 0.7149\n",
      "Epoch 14/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.5450 - acc: 0.7993 - val_loss: 0.5505 - val_acc: 0.7777\n",
      "Epoch 15/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.5231 - acc: 0.7993 - val_loss: 0.5462 - val_acc: 0.7849\n",
      "Epoch 16/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.4919 - acc: 0.8190 - val_loss: 0.5124 - val_acc: 0.8004\n",
      "Epoch 17/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.4839 - acc: 0.8149 - val_loss: 0.5189 - val_acc: 0.8038\n",
      "Epoch 18/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.4868 - acc: 0.8180 - val_loss: 0.5178 - val_acc: 0.7979\n",
      "Epoch 19/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.4682 - acc: 0.8265 - val_loss: 0.5255 - val_acc: 0.8006\n",
      "Epoch 20/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.4582 - acc: 0.8316 - val_loss: 0.5317 - val_acc: 0.7972\n",
      "Epoch 21/80\n",
      "6093/6093 [==============================] - 9s 1ms/step - loss: 0.4403 - acc: 0.8374 - val_loss: 0.5271 - val_acc: 0.8080\n",
      "Epoch 22/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.4222 - acc: 0.8475 - val_loss: 0.5060 - val_acc: 0.8112\n",
      "Epoch 23/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.4240 - acc: 0.8447 - val_loss: 0.4899 - val_acc: 0.8125\n",
      "Epoch 24/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.4124 - acc: 0.8472 - val_loss: 0.4829 - val_acc: 0.8216\n",
      "Epoch 25/80\n",
      "6093/6093 [==============================] - 9s 1ms/step - loss: 0.3972 - acc: 0.8589 - val_loss: 0.4820 - val_acc: 0.8211\n",
      "Epoch 26/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.4088 - acc: 0.8533 - val_loss: 0.5134 - val_acc: 0.8139\n",
      "Epoch 27/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.3930 - acc: 0.8600 - val_loss: 0.5079 - val_acc: 0.8130\n",
      "Epoch 28/80\n",
      "6093/6093 [==============================] - 9s 1ms/step - loss: 0.3913 - acc: 0.8595 - val_loss: 0.4878 - val_acc: 0.8231\n",
      "Epoch 29/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.3705 - acc: 0.8671 - val_loss: 0.4736 - val_acc: 0.8221\n",
      "Epoch 30/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.3699 - acc: 0.8679 - val_loss: 0.5031 - val_acc: 0.8159\n",
      "Epoch 31/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.3619 - acc: 0.8666 - val_loss: 0.6084 - val_acc: 0.8053\n",
      "Epoch 32/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.3595 - acc: 0.8690 - val_loss: 0.5033 - val_acc: 0.8189\n",
      "Epoch 33/80\n",
      "6093/6093 [==============================] - 9s 1ms/step - loss: 0.3580 - acc: 0.8718 - val_loss: 0.4913 - val_acc: 0.8194\n",
      "Epoch 34/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.3435 - acc: 0.8799 - val_loss: 0.5391 - val_acc: 0.8068\n",
      "Epoch 35/80\n",
      "6093/6093 [==============================] - 9s 1ms/step - loss: 0.3413 - acc: 0.8792 - val_loss: 0.4979 - val_acc: 0.8176\n",
      "Epoch 36/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.3391 - acc: 0.8808 - val_loss: 0.5352 - val_acc: 0.8250\n",
      "Epoch 37/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.3373 - acc: 0.8820 - val_loss: 0.4968 - val_acc: 0.8310\n",
      "Epoch 38/80\n",
      "6093/6093 [==============================] - 9s 1ms/step - loss: 0.3200 - acc: 0.8902 - val_loss: 0.5012 - val_acc: 0.8260\n",
      "Epoch 39/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.3218 - acc: 0.8849 - val_loss: 0.5305 - val_acc: 0.8112\n",
      "Epoch 40/80\n",
      "6093/6093 [==============================] - 9s 1ms/step - loss: 0.3210 - acc: 0.8853 - val_loss: 0.4981 - val_acc: 0.8216\n",
      "Epoch 41/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.3031 - acc: 0.8922 - val_loss: 0.5413 - val_acc: 0.8248\n",
      "Epoch 42/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.3019 - acc: 0.8936 - val_loss: 0.5521 - val_acc: 0.8221\n",
      "Epoch 43/80\n",
      "6093/6093 [==============================] - 9s 1ms/step - loss: 0.2949 - acc: 0.8997 - val_loss: 0.5131 - val_acc: 0.8194\n",
      "Epoch 44/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.3058 - acc: 0.8899 - val_loss: 0.5437 - val_acc: 0.8090\n",
      "Epoch 45/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.2822 - acc: 0.9032 - val_loss: 0.5225 - val_acc: 0.8329\n",
      "Epoch 46/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.2734 - acc: 0.9040 - val_loss: 0.5496 - val_acc: 0.8223\n",
      "Epoch 47/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.2749 - acc: 0.9043 - val_loss: 0.5710 - val_acc: 0.8233\n",
      "Epoch 48/80\n",
      "6093/6093 [==============================] - 9s 1ms/step - loss: 0.2638 - acc: 0.9094 - val_loss: 0.5504 - val_acc: 0.8317\n",
      "Epoch 49/80\n",
      "6093/6093 [==============================] - 8s 1ms/step - loss: 0.2612 - acc: 0.9084 - val_loss: 0.5628 - val_acc: 0.8221\n",
      "4058/4058 [==============================] - 2s 422us/step\n",
      "[0.73149492 0.65789474 0.892728   0.70868978]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import itertools\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import f1_score\n",
    "import tools_split_60_40 as tools\n",
    "\n",
    "training_data, training_label, validation_data, validation_label, validation_cate_label = tools.get_data()\n",
    "\n",
    "kernel_size = [5]\n",
    "num_layers = [10]\n",
    "batch_size = [30]\n",
    "learning_rate = [0.0001]\n",
    "overflow_model = 0\n",
    "\n",
    "def run(bs, lr, ks, num_layer):\n",
    "    model = tools.create_model(lr, bs, ks, num_layer)\n",
    "    early_stop = EarlyStopping(patience=20)\n",
    "    history = model.fit(x = training_data, \n",
    "                        y = training_label,\n",
    "                        epochs=80,\n",
    "                        validation_data=(validation_data, validation_label),\n",
    "                        callbacks=[early_stop],\n",
    "                        batch_size=bs, \n",
    "                        verbose=1)\n",
    "    evaluation = model.evaluate(x = validation_data, y = validation_label)\n",
    "    validation_prediction = model.predict_classes(validation_data, batch_size=bs)\n",
    "    score = f1_score(validation_cate_label, validation_prediction, average=None)\n",
    "    \n",
    "    del model\n",
    "    return score\n",
    "\n",
    "## main()\n",
    "\n",
    "index=1\n",
    "for ks, num_layer, bs, lr in itertools.product(kernel_size, num_layers, batch_size, learning_rate):\n",
    "    print(\"Index \"+str(index))\n",
    "    \n",
    "    model = tools.create_model(lr, bs, ks, num_layer)\n",
    "    \n",
    "    score = run(bs, lr, ks, num_layer)\n",
    "    print(score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 90:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 903\n",
      "~: 299\n",
      "O: 2990\n",
      "N: 5959\n",
      "split train data index:  [[0, 813, 904, 903], [903, 1173, 1203, 1202], [1202, 3894, 4194, 4192], [4192, 9556, 10152, 10151]]\n",
      "(9139, 9000)\n",
      "(9139,)\n",
      "(1012, 9000)\n",
      "(1012,)\n",
      "(0,)\n",
      "(0,)\n",
      "Train Data: (9139, 9000, 1)\n",
      "Train Label:  (9139, 4)\n",
      "Train Data: (1012, 9000, 1)\n",
      "Train Label:  (1012, 4)\n",
      "Index 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 8996, 32)          192       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 8996, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8996, 32)          128       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 4498, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 4494, 32)          5152      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4494, 32)          0         \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 2247, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 2243, 64)          10304     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2243, 64)          0         \n",
      "_________________________________________________________________\n",
      "average_pooling1d_3 (Average (None, 1121, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 1117, 64)          20544     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1117, 64)          0         \n",
      "_________________________________________________________________\n",
      "average_pooling1d_4 (Average (None, 558, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 554, 128)          41088     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 554, 128)          0         \n",
      "_________________________________________________________________\n",
      "average_pooling1d_5 (Average (None, 277, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 273, 128)          82048     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 273, 128)          0         \n",
      "_________________________________________________________________\n",
      "average_pooling1d_6 (Average (None, 136, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 136, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 132, 256)          164096    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 132, 256)          0         \n",
      "_________________________________________________________________\n",
      "average_pooling1d_7 (Average (None, 66, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 62, 256)           327936    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 62, 256)           0         \n",
      "_________________________________________________________________\n",
      "average_pooling1d_8 (Average (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 27, 512)           655872    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 27, 512)           0         \n",
      "_________________________________________________________________\n",
      "average_pooling1d_9 (Average (None, 13, 512)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 13, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 9, 512)            1311232   \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 9, 512)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 3,212,804\n",
      "Trainable params: 3,212,740\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 9139 samples, validate on 1012 samples\n",
      "Epoch 1/80\n",
      "9139/9139 [==============================] - 20s 2ms/step - loss: 1.0425 - acc: 0.5695 - val_loss: 0.9592 - val_acc: 0.5879\n",
      "Epoch 2/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.9476 - acc: 0.5866 - val_loss: 0.8794 - val_acc: 0.6107\n",
      "Epoch 3/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.8786 - acc: 0.6111 - val_loss: 0.8464 - val_acc: 0.6601\n",
      "Epoch 4/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.8081 - acc: 0.6579 - val_loss: 0.7417 - val_acc: 0.6858\n",
      "Epoch 5/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.7395 - acc: 0.6869 - val_loss: 0.6861 - val_acc: 0.7134\n",
      "Epoch 6/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.6914 - acc: 0.7117 - val_loss: 0.6444 - val_acc: 0.7372\n",
      "Epoch 7/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.6440 - acc: 0.7307 - val_loss: 0.6086 - val_acc: 0.7233\n",
      "Epoch 8/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.6140 - acc: 0.7472 - val_loss: 0.5872 - val_acc: 0.7540\n",
      "Epoch 9/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.5695 - acc: 0.7743 - val_loss: 0.5733 - val_acc: 0.7520\n",
      "Epoch 10/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.5376 - acc: 0.7922 - val_loss: 0.5069 - val_acc: 0.8043\n",
      "Epoch 11/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.5074 - acc: 0.8102 - val_loss: 0.5004 - val_acc: 0.8132\n",
      "Epoch 12/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.4887 - acc: 0.8146 - val_loss: 0.5203 - val_acc: 0.8113\n",
      "Epoch 13/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.4769 - acc: 0.8211 - val_loss: 0.4818 - val_acc: 0.8192\n",
      "Epoch 14/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.4644 - acc: 0.8306 - val_loss: 0.5329 - val_acc: 0.7974\n",
      "Epoch 15/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.4533 - acc: 0.8347 - val_loss: 0.4610 - val_acc: 0.8202\n",
      "Epoch 16/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.4367 - acc: 0.8443 - val_loss: 0.4692 - val_acc: 0.8221\n",
      "Epoch 17/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.4373 - acc: 0.8407 - val_loss: 0.4776 - val_acc: 0.8172\n",
      "Epoch 18/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.4316 - acc: 0.8429 - val_loss: 0.4719 - val_acc: 0.8202\n",
      "Epoch 19/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.4176 - acc: 0.8469 - val_loss: 0.4912 - val_acc: 0.8152\n",
      "Epoch 20/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.4088 - acc: 0.8476 - val_loss: 0.4644 - val_acc: 0.8281\n",
      "Epoch 21/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.4082 - acc: 0.8504 - val_loss: 0.4508 - val_acc: 0.8310\n",
      "Epoch 22/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.3997 - acc: 0.8572 - val_loss: 0.4507 - val_acc: 0.8320\n",
      "Epoch 23/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.3899 - acc: 0.8570 - val_loss: 0.4739 - val_acc: 0.8142\n",
      "Epoch 24/80\n",
      "9139/9139 [==============================] - 10s 1ms/step - loss: 0.3978 - acc: 0.8569 - val_loss: 0.4578 - val_acc: 0.8261\n",
      "Epoch 25/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.3815 - acc: 0.8644 - val_loss: 0.4287 - val_acc: 0.8449\n",
      "Epoch 26/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.3784 - acc: 0.8618 - val_loss: 0.4404 - val_acc: 0.8350\n",
      "Epoch 27/80\n",
      "9139/9139 [==============================] - 10s 1ms/step - loss: 0.3717 - acc: 0.8680 - val_loss: 0.4524 - val_acc: 0.8350\n",
      "Epoch 28/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.3622 - acc: 0.8719 - val_loss: 0.4268 - val_acc: 0.8370\n",
      "Epoch 29/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.3568 - acc: 0.8695 - val_loss: 0.4458 - val_acc: 0.8310\n",
      "Epoch 30/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.3513 - acc: 0.8736 - val_loss: 0.4645 - val_acc: 0.8221\n",
      "Epoch 31/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.3475 - acc: 0.8744 - val_loss: 0.4907 - val_acc: 0.8192\n",
      "Epoch 32/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.3484 - acc: 0.8746 - val_loss: 0.4625 - val_acc: 0.8251\n",
      "Epoch 33/80\n",
      "9139/9139 [==============================] - 10s 1ms/step - loss: 0.3415 - acc: 0.8777 - val_loss: 0.4625 - val_acc: 0.8379\n",
      "Epoch 34/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.3344 - acc: 0.8809 - val_loss: 0.4541 - val_acc: 0.8370\n",
      "Epoch 35/80\n",
      "9139/9139 [==============================] - 10s 1ms/step - loss: 0.3298 - acc: 0.8834 - val_loss: 0.4715 - val_acc: 0.8330\n",
      "Epoch 36/80\n",
      "9139/9139 [==============================] - 10s 1ms/step - loss: 0.3213 - acc: 0.8863 - val_loss: 0.4872 - val_acc: 0.8211\n",
      "Epoch 37/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.3100 - acc: 0.8889 - val_loss: 0.4629 - val_acc: 0.8281\n",
      "Epoch 38/80\n",
      "9139/9139 [==============================] - 10s 1ms/step - loss: 0.3091 - acc: 0.8904 - val_loss: 0.4450 - val_acc: 0.8379\n",
      "Epoch 39/80\n",
      "9139/9139 [==============================] - 10s 1ms/step - loss: 0.3084 - acc: 0.8903 - val_loss: 0.4910 - val_acc: 0.8162\n",
      "Epoch 40/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.2989 - acc: 0.8957 - val_loss: 0.4776 - val_acc: 0.8251\n",
      "Epoch 41/80\n",
      "9139/9139 [==============================] - 10s 1ms/step - loss: 0.2972 - acc: 0.8931 - val_loss: 0.4623 - val_acc: 0.8419\n",
      "Epoch 42/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.2976 - acc: 0.8931 - val_loss: 0.4649 - val_acc: 0.8320\n",
      "Epoch 43/80\n",
      "9139/9139 [==============================] - 10s 1ms/step - loss: 0.2831 - acc: 0.9014 - val_loss: 0.4851 - val_acc: 0.8379\n",
      "Epoch 44/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.2831 - acc: 0.9009 - val_loss: 0.4826 - val_acc: 0.8350\n",
      "Epoch 45/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.2753 - acc: 0.9033 - val_loss: 0.5015 - val_acc: 0.8291\n",
      "Epoch 46/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.2696 - acc: 0.9075 - val_loss: 0.5029 - val_acc: 0.8330\n",
      "Epoch 47/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.2667 - acc: 0.9058 - val_loss: 0.5030 - val_acc: 0.8192\n",
      "Epoch 48/80\n",
      "9139/9139 [==============================] - 11s 1ms/step - loss: 0.2643 - acc: 0.9094 - val_loss: 0.5382 - val_acc: 0.7994\n",
      "1012/1012 [==============================] - 0s 463us/step\n",
      "[0.72489083 0.62162162 0.87796313 0.6975945 ]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import itertools\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import f1_score\n",
    "import tools_split_90_10 as tools\n",
    "\n",
    "training_data, training_label, validation_data, validation_label, validation_cate_label = tools.get_data()\n",
    "\n",
    "kernel_size = [5]\n",
    "num_layers = [10]\n",
    "batch_size = [30]\n",
    "learning_rate = [0.0001]\n",
    "overflow_model = 0\n",
    "\n",
    "def run(bs, lr, ks, num_layer):\n",
    "    model = tools.create_model(lr, bs, ks, num_layer)\n",
    "    early_stop = EarlyStopping(patience=20)\n",
    "    history = model.fit(x = training_data, \n",
    "                        y = training_label,\n",
    "                        epochs=80,\n",
    "                        validation_data=(validation_data, validation_label),\n",
    "                        callbacks=[early_stop],\n",
    "                        batch_size=bs, \n",
    "                        verbose=1)\n",
    "    evaluation = model.evaluate(x = validation_data, y = validation_label)\n",
    "    validation_prediction = model.predict_classes(validation_data, batch_size=bs)\n",
    "    score = f1_score(validation_cate_label, validation_prediction, average=None)\n",
    "    \n",
    "    del model\n",
    "    return score\n",
    "\n",
    "## main()\n",
    "\n",
    "index=1\n",
    "for ks, num_layer, bs, lr in itertools.product(kernel_size, num_layers, batch_size, learning_rate):\n",
    "    print(\"Index \"+str(index))\n",
    "    \n",
    "    model = tools.create_model(lr, bs, ks, num_layer)\n",
    "    print(model.summary())\n",
    "    score = run(bs, lr, ks, num_layer)\n",
    "    print(score)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
